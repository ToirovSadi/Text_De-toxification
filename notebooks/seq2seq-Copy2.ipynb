{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51b0681a-69e2-4f9f-9ed4-01c96556980d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_MODEL = 'models/seq2seq.01.pt'\n",
    "MODEL_CHECKPOINT = 'models/seq2seq.01.pt'\n",
    "DATASET_PATH = 'data/interim/preprocessed_paranmt3.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40068414-5b9f-4586-bcdb-470c3efb7c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os\n",
    "os.chdir(\"..\") # go to the root dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd03cca-29af-4e00-865b-e0c255cc787f",
   "metadata": {},
   "source": [
    "# Get the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4a86d33-7970-403b-b0e7-601c13db61ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENT_SIZE = 12\n",
    "MAX_TOKENS = 30_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e44e86ca-4ca7-4594-9ce7-3dc4c457cf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.make_dataset import ParanmtDataset\n",
    "\n",
    "train_dataset = ParanmtDataset(\n",
    "    path=DATASET_PATH,\n",
    "    max_sent_size=MAX_SENT_SIZE,\n",
    "    train=True,\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3822e15e-e711-43c1-a0c2-5e912b406de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.build_vocab(\n",
    "    min_freq=2,\n",
    "    specials=['<unk>', '<pad>', '<sos>', '<eos>'],\n",
    "    max_tokens=MAX_TOKENS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "546d87b2-3b10-4951-ac9e-fa936f2ef204",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_vocab = train_dataset.toxic_vocab\n",
    "dec_vocab = train_dataset.neutral_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc99d633-b8bc-4ae0-b56d-78ec8642c42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of encoder vocab: 16608\n",
      "size of decoder vocab: 19520\n"
     ]
    }
   ],
   "source": [
    "print(\"size of encoder vocab:\", len(enc_vocab))\n",
    "print(\"size of decoder vocab:\", len(dec_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a7b0469-8585-4289-9e45-5f62495f27d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = ParanmtDataset(\n",
    "    path=DATASET_PATH,\n",
    "    max_sent_size=MAX_SENT_SIZE,\n",
    "    vocabs=(enc_vocab, dec_vocab), # avoid data leakage\n",
    "    train=False,\n",
    "    seed=42,\n",
    "    take_first=10_000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7abeadd3-8c26-4b70-987b-be5e16ebdf6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>similarity</th>\n",
       "      <th>lenght_diff</th>\n",
       "      <th>toxic_sent</th>\n",
       "      <th>neutral_sent</th>\n",
       "      <th>toxic_val</th>\n",
       "      <th>neutral_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.607065</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>[get, yourself, a, good, hooker, .]</td>\n",
       "      <td>[go, get, yourself, a, nice, bird, .]</td>\n",
       "      <td>0.999393</td>\n",
       "      <td>0.001473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.918913</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>[long, hair, ,, pretty, little, mouth, ,, perf...</td>\n",
       "      <td>[long, hair, ,, nice, lips, ,, perfect, butt, ?]</td>\n",
       "      <td>0.997611</td>\n",
       "      <td>0.106524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.634488</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>[you, should, have, finished, it, and, ordered...</td>\n",
       "      <td>[you, should, have, ordered, that, pilot, to, ...</td>\n",
       "      <td>0.953872</td>\n",
       "      <td>0.014506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.865425</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>[what, the, hell, is, he, saying, ?]</td>\n",
       "      <td>[what, does, he, say, ?]</td>\n",
       "      <td>0.967230</td>\n",
       "      <td>0.000041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.845061</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>[then, why, are, you, crazy, ?]</td>\n",
       "      <td>[then, why, with, the, crazy, ?]</td>\n",
       "      <td>0.994152</td>\n",
       "      <td>0.007104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238619</th>\n",
       "      <td>0.809637</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[parasites, kill, puppies, .]</td>\n",
       "      <td>[germs, can, kill, puppies, .]</td>\n",
       "      <td>0.997600</td>\n",
       "      <td>0.001762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238620</th>\n",
       "      <td>0.660608</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>[damn, it, ,, where, is, everybody, ?, bruno, !]</td>\n",
       "      <td>[where, the, hell, did, everyone, go, ?]</td>\n",
       "      <td>0.997657</td>\n",
       "      <td>0.176493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238621</th>\n",
       "      <td>0.697438</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>[i, am, tired, of, hearing, you, two, gallinas...</td>\n",
       "      <td>[i, am, tired, of, hearing, your, whining, .]</td>\n",
       "      <td>0.955657</td>\n",
       "      <td>0.273435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238622</th>\n",
       "      <td>0.711497</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>[it, is, a, blow, job, .]</td>\n",
       "      <td>[she, is, a, whack, job, .]</td>\n",
       "      <td>0.994690</td>\n",
       "      <td>0.192220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238623</th>\n",
       "      <td>0.914001</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>[you, are, still, clinging, to, your, fake, li...</td>\n",
       "      <td>[you, are, still, clinging, to, a, false, life...</td>\n",
       "      <td>0.883707</td>\n",
       "      <td>0.006868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>238624 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        similarity  lenght_diff  \\\n",
       "0         0.607065     0.034483   \n",
       "1         0.918913     0.200000   \n",
       "2         0.634488     0.185185   \n",
       "3         0.865425     0.357143   \n",
       "4         0.845061     0.040000   \n",
       "...            ...          ...   \n",
       "238619    0.809637     0.000000   \n",
       "238620    0.660608     0.111111   \n",
       "238621    0.697438     0.222222   \n",
       "238622    0.711497     0.105263   \n",
       "238623    0.914001     0.048780   \n",
       "\n",
       "                                               toxic_sent  \\\n",
       "0                     [get, yourself, a, good, hooker, .]   \n",
       "1       [long, hair, ,, pretty, little, mouth, ,, perf...   \n",
       "2       [you, should, have, finished, it, and, ordered...   \n",
       "3                    [what, the, hell, is, he, saying, ?]   \n",
       "4                         [then, why, are, you, crazy, ?]   \n",
       "...                                                   ...   \n",
       "238619                      [parasites, kill, puppies, .]   \n",
       "238620   [damn, it, ,, where, is, everybody, ?, bruno, !]   \n",
       "238621  [i, am, tired, of, hearing, you, two, gallinas...   \n",
       "238622                          [it, is, a, blow, job, .]   \n",
       "238623  [you, are, still, clinging, to, your, fake, li...   \n",
       "\n",
       "                                             neutral_sent  toxic_val  \\\n",
       "0                   [go, get, yourself, a, nice, bird, .]   0.999393   \n",
       "1        [long, hair, ,, nice, lips, ,, perfect, butt, ?]   0.997611   \n",
       "2       [you, should, have, ordered, that, pilot, to, ...   0.953872   \n",
       "3                                [what, does, he, say, ?]   0.967230   \n",
       "4                        [then, why, with, the, crazy, ?]   0.994152   \n",
       "...                                                   ...        ...   \n",
       "238619                     [germs, can, kill, puppies, .]   0.997600   \n",
       "238620           [where, the, hell, did, everyone, go, ?]   0.997657   \n",
       "238621      [i, am, tired, of, hearing, your, whining, .]   0.955657   \n",
       "238622                        [she, is, a, whack, job, .]   0.994690   \n",
       "238623  [you, are, still, clinging, to, a, false, life...   0.883707   \n",
       "\n",
       "        neutral_val  \n",
       "0          0.001473  \n",
       "1          0.106524  \n",
       "2          0.014506  \n",
       "3          0.000041  \n",
       "4          0.007104  \n",
       "...             ...  \n",
       "238619     0.001762  \n",
       "238620     0.176493  \n",
       "238621     0.273435  \n",
       "238622     0.192220  \n",
       "238623     0.006868  \n",
       "\n",
       "[238624 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1688b81e-8e3c-4266-8ab9-07f52124e827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(238624, 10000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd8b060-2108-4d18-865c-f9de2ae7969c",
   "metadata": {},
   "source": [
    "# Build the Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f82c34f-8f03-45ae-8eae-02dd2759bf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7d5e2e4-dce6-470f-9f5d-06a09c798d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7953a61f-64c0-49a3-9fda-f64e80861fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic_sent.shape: torch.Size([32, 12])\n",
      "neutral_sent.shape: torch.Size([32, 12])\n"
     ]
    }
   ],
   "source": [
    "# let's check if shape and everything is ok\n",
    "for batch in train_dataloader:\n",
    "    toxic_sent, neutral_sent = batch\n",
    "    print(\"toxic_sent.shape:\", toxic_sent.shape)\n",
    "    print(\"neutral_sent.shape:\", neutral_sent.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dca808e2-529f-4c8e-b035-3b03ec7d053d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc46dfbe-d834-48e7-ad09-c49c2a676386",
   "metadata": {},
   "source": [
    "# Load the Model\n",
    "\n",
    "- Simple EncoderDecoder (Seq2Seq) architerture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08d88aaa-7e97-411f-9164-4bdbfae09a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.seq2seq.encoder import Encoder\n",
    "from src.models.seq2seq.decoder import Decoder\n",
    "from src.models.seq2seq import Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "451a159a-dae5-487c-be50-c87b7407add1",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(enc_vocab)\n",
    "OUTPUT_DIM = len(dec_vocab)\n",
    "EMBED_DIM = 128\n",
    "NUM_HIDDEN = 256\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.3\n",
    "ENC_PADDING_IDX = enc_vocab['<pad>']\n",
    "DEC_PADDING_IDX = dec_vocab['<pad>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b48dfdf-ba48-477c-ac62-2c512a69d86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the encoder and decoder for our model\n",
    "encoder = Encoder(\n",
    "    input_dim=INPUT_DIM,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    hidden_dim=NUM_HIDDEN,\n",
    "    num_layers=N_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    vocab=enc_vocab,\n",
    "    padding_idx=ENC_PADDING_IDX\n",
    ").to(device)\n",
    "\n",
    "decoder = Decoder(\n",
    "    output_dim=OUTPUT_DIM,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    hidden_dim=NUM_HIDDEN,\n",
    "    num_layers=N_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    vocab=dec_vocab,\n",
    "    padding_idx=DEC_PADDING_IDX\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb68fa7c-f8ee-4667-9b20-a638de4fe311",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = float('inf')\n",
    "\n",
    "model = Seq2Seq(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    device=device,\n",
    "    max_sent_size=MAX_SENT_SIZE,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c0d4feb-d171-4c6d-99dd-d05fb9e804c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters in model: 26.0M\n"
     ]
    }
   ],
   "source": [
    "from src.models.utils import count_parameters\n",
    "\n",
    "print(f\"number of parameters in model: {count_parameters(model)//1e6}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "088f446e-d95e-4812-949d-a3a5eb20ab6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=dec_vocab['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae355174-b99f-403c-b926-def9bd694c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7457/7457 [04:00<00:00, 31.05it/s, loss=3.76]\n",
      "Evaluating 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [00:02<00:00, 143.49it/s, loss=5.42]\n",
      "Training 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7457/7457 [03:59<00:00, 31.13it/s, loss=3.13]\n",
      "Evaluating 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [00:02<00:00, 154.96it/s, loss=5.49]\n",
      "Training 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7457/7457 [04:00<00:00, 30.98it/s, loss=2.89]\n",
      "Evaluating 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [00:02<00:00, 145.94it/s, loss=5.65]\n",
      "Training 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7457/7457 [04:02<00:00, 30.73it/s, loss=2.75]\n",
      "Evaluating 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [00:02<00:00, 148.30it/s, loss=5.67]\n",
      "Training 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7457/7457 [03:59<00:00, 31.10it/s, loss=2.64]\n",
      "Evaluating 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [00:02<00:00, 146.70it/s, loss=5.76]\n",
      "Training 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7457/7457 [04:00<00:00, 31.06it/s, loss=2.57]\n",
      "Evaluating 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [00:02<00:00, 146.59it/s, loss=5.86]\n",
      "Training 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7457/7457 [04:00<00:00, 30.98it/s, loss=2.5] \n",
      "Evaluating 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [00:02<00:00, 151.93it/s, loss=5.97]\n",
      "Training 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7457/7457 [04:02<00:00, 30.81it/s, loss=2.45]\n",
      "Evaluating 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [00:02<00:00, 147.24it/s, loss=5.93]\n",
      "Training 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7457/7457 [04:00<00:00, 31.06it/s, loss=2.4] \n",
      "Evaluating 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [00:02<00:00, 138.14it/s, loss=5.97]\n",
      "Training 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7457/7457 [03:59<00:00, 31.14it/s, loss=2.36]\n",
      "Evaluating 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [00:02<00:00, 147.50it/s, loss=6]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update teacher force to 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7457/7457 [03:58<00:00, 31.31it/s, loss=3.67]\n",
      "Evaluating 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [00:02<00:00, 150.89it/s, loss=3.69]\n",
      "Training 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7457/7457 [04:01<00:00, 30.94it/s, loss=3.5] \n",
      "Evaluating 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [00:02<00:00, 144.02it/s, loss=3.62]\n",
      "Training 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7457/7457 [03:58<00:00, 31.23it/s, loss=3.41]\n",
      "Evaluating 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [00:02<00:00, 150.61it/s, loss=3.63]\n",
      "Training 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7457/7457 [03:58<00:00, 31.32it/s, loss=3.35]\n",
      "Evaluating 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [00:02<00:00, 143.69it/s, loss=3.59]\n",
      "Training 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7457/7457 [03:59<00:00, 31.19it/s, loss=3.3] \n",
      "Evaluating 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [00:02<00:00, 143.53it/s, loss=3.58]\n",
      "Training 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7457/7457 [04:00<00:00, 30.99it/s, loss=3.26]\n",
      "Evaluating 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [00:02<00:00, 145.06it/s, loss=3.56]\n",
      "Training 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7457/7457 [03:58<00:00, 31.32it/s, loss=3.22]\n",
      "Evaluating 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [00:02<00:00, 144.46it/s, loss=3.54]\n",
      "Training 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7457/7457 [03:58<00:00, 31.28it/s, loss=3.19]\n",
      "Evaluating 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [00:02<00:00, 149.71it/s, loss=3.55]\n",
      "Training 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7457/7457 [03:58<00:00, 31.30it/s, loss=3.16]\n",
      "Evaluating 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [00:02<00:00, 143.17it/s, loss=3.51]\n",
      "Training 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7457/7457 [04:00<00:00, 30.95it/s, loss=3.13]\n",
      "Evaluating 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [00:02<00:00, 145.88it/s, loss=3.53]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update teacher force to 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 21: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7457/7457 [03:59<00:00, 31.13it/s, loss=3.1] \n",
      "Evaluating 21: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [00:02<00:00, 145.29it/s, loss=3.52]\n",
      "Training 22: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7457/7457 [03:58<00:00, 31.25it/s, loss=3.08]\n",
      "Evaluating 22: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [00:02<00:00, 144.49it/s, loss=3.5] \n",
      "Training 23: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7457/7457 [03:57<00:00, 31.35it/s, loss=3.05]\n",
      "Evaluating 23: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [00:02<00:00, 145.59it/s, loss=3.51]\n",
      "Training 24: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7457/7457 [04:01<00:00, 30.92it/s, loss=3.03]\n",
      "Evaluating 24: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [00:02<00:00, 144.18it/s, loss=3.49]\n",
      "Training 25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7457/7457 [03:58<00:00, 31.30it/s, loss=3.01]\n",
      "Evaluating 25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [00:02<00:00, 141.95it/s, loss=3.5] \n",
      "Training 26: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7457/7457 [03:58<00:00, 31.24it/s, loss=2.99]\n",
      "Evaluating 26: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [00:02<00:00, 141.40it/s, loss=3.52]\n",
      "Training 27: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7457/7457 [03:58<00:00, 31.23it/s, loss=2.97]\n",
      "Evaluating 27: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [00:02<00:00, 144.38it/s, loss=3.48]\n",
      "Training 28: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7457/7457 [04:00<00:00, 30.96it/s, loss=2.95]\n",
      "Evaluating 28: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [00:02<00:00, 146.88it/s, loss=3.47]\n",
      "Training 29: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7457/7457 [03:59<00:00, 31.15it/s, loss=2.94]\n",
      "Evaluating 29: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [00:02<00:00, 140.05it/s, loss=3.49]\n",
      "Training 30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7457/7457 [03:59<00:00, 31.17it/s, loss=2.92]\n",
      "Evaluating 30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [00:02<00:00, 146.86it/s, loss=3.47]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update teacher force to 0\n"
     ]
    }
   ],
   "source": [
    "from src.models.train_model import train\n",
    "\n",
    "best_loss = train(\n",
    "    model=model,\n",
    "    loaders=(train_dataloader, val_dataloader),\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    epochs=30,\n",
    "    device=device,\n",
    "    best_loss=best_loss,\n",
    "    ckpt_path=MODEL_CHECKPOINT,\n",
    "    clip_grad=1,\n",
    "    teacher_force={\n",
    "        'value': 1,\n",
    "        'gamma': 0,\n",
    "        'update_every_n_epoch': 10,\n",
    "    } # first 10 epoch teacher force 1, after it will be turned off\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd75631f-1539-4768-a006-9ad9315541bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.466131195854455"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7dc1b6ad-901d-4582-84c8-79ba34fdcebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (vocab): Vocab()\n",
       "    (embedding): Embedding(16608, 128, padding_idx=1)\n",
       "    (rnn): GRU(128, 256, num_layers=2, batch_first=True, dropout=0.3)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (vocab): Vocab()\n",
       "    (embedding): Embedding(19520, 128, padding_idx=1)\n",
       "    (rnn): GRU(128, 256, num_layers=2, batch_first=True, dropout=0.3)\n",
       "    (fc_out): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.3, inplace=False)\n",
       "      (3): Linear(in_features=1024, out_features=19520, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's load the model and predict\n",
    "model = torch.load(MODEL_CHECKPOINT)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3f96417-66e1-4061-ba49-55207e953142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic_sent: what the fuck he is talking about?!\n",
      "neutral_sent: what is he talking about?\n",
      "predictions:\n",
      "\t1) what is he talking about?\n",
      "\t2) what is he talking about about\n",
      "\t3) what is he talking talking?\n",
      "\n",
      "\n",
      "toxic_sent: it is fucking painful.\n",
      "neutral_sent: it hurts a lot.\n",
      "predictions:\n",
      "\t1) it is bloody.\n",
      "\t2) it is fucking . .\n",
      "\t3) it is bloody . .\n",
      "\n",
      "\n",
      "toxic_sent: we can fuck tomorrow.\n",
      "neutral_sent: we can love each other tomorrow.\n",
      "predictions:\n",
      "\t1) we can die tomorrow tomorrow tomorrow.\n",
      "\t2) we can die tomorrow tomorrow . .\n",
      "\t3) we can die tomorrow tomorrow tomorrow . .\n",
      "\n",
      "\n",
      "toxic_sent: i fucking told you.\n",
      "neutral_sent: i already told you.\n",
      "predictions:\n",
      "\t1) i told you.\n",
      "\t2) i told you!\n",
      "\t3) i told you . .\n",
      "\n",
      "\n",
      "toxic_sent: studying torah . asshole,\n",
      "neutral_sent: i am studying the torah, piping.\n",
      "predictions:\n",
      "\t1) happy, . . . . . . .\n",
      "\t2) happy, . . . . . .\n",
      "\t3) happy, . . . . .\n",
      "\n",
      "\n",
      "toxic_sent: you get one shot.\n",
      "neutral_sent: you have one chance.\n",
      "predictions:\n",
      "\t1) you shot one chance.\n",
      "\t2) you shot one shot.\n",
      "\t3) you have one chance.\n",
      "\n",
      "\n",
      "toxic_sent: had we deteriorated to the level of dumb beasts?\n",
      "neutral_sent: have we degenerated into poor animals' levels?\n",
      "predictions:\n",
      "\t1) did we agree the the the the????\n",
      "\t2) were we agree to the the the???\n",
      "\t3) did we agree the the the the???\n",
      "\n",
      "\n",
      "toxic_sent: you have some shit on your pants.\n",
      "neutral_sent: some schmutz on your pants.\n",
      "predictions:\n",
      "\t1) you have got in pants pants.\n",
      "\t2) you have got pants pants pants.\n",
      "\t3) you have got in pants pants . .\n",
      "\n",
      "\n",
      "toxic_sent: the scary witch.\n",
      "neutral_sent: the scary witch! look out!\n",
      "predictions:\n",
      "\t1) the wicked . . .\n",
      "\t2) the wicked . .\n",
      "\t3) the witch . . .\n",
      "\n",
      "\n",
      "toxic_sent: the scum will not be tolerated.\n",
      "neutral_sent: riffraff shall not be tolerated.\n",
      "predictions:\n",
      "\t1) the is will not be . . . . .\n",
      "\t2) the is will not be . .\n",
      "\t3) the is will not be . . .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "detokenizer = TreebankWordDetokenizer()\n",
    "\n",
    "# let's see how our model works\n",
    "num_examples = 10\n",
    "num_sentence = 3\n",
    "dataset = train_dataset\n",
    "for _ in range(num_examples):\n",
    "    idx = np.random.randint(0, len(dataset))\n",
    "    toxic_sent = detokenizer.detokenize(dataset.df.loc[idx, 'toxic_sent'])\n",
    "    neutral_sent = detokenizer.detokenize(dataset.df.loc[idx, 'neutral_sent'])\n",
    "    \n",
    "    print('toxic_sent:', toxic_sent)\n",
    "    print('neutral_sent:', neutral_sent)\n",
    "    preds = model.predict(toxic_sent, beam=True, beam_search_num_sentence=num_sentence) # let's use beam search\n",
    "    print(\"predictions:\")\n",
    "    for i in range(num_sentence):\n",
    "        print(f\"\\t{i+1})\", preds[i])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1114d455-7397-41cf-8e33-03584625a4c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
