{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad2118f3-61a9-4655-bde9-b26e97cc7155",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_MODEL = 'models/transformer2.01.pt'\n",
    "MODEL_CHECKPOINT = 'models/transformer2.01.pt'\n",
    "DATASET_PATH = 'data/interim/preprocessed_paranmt.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40068414-5b9f-4586-bcdb-470c3efb7c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "import os\n",
    "os.chdir(\"..\") # go to the root dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd8b060-2108-4d18-865c-f9de2ae7969c",
   "metadata": {},
   "source": [
    "## Get the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c079f37c-1954-4a89-b159-84b750279697",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENT_SIZE = 32\n",
    "MAX_TOKENS = 10_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d26e343f-e42b-442e-9bb4-0a0d96bbf4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.make_dataset import ParanmtDataset\n",
    "\n",
    "train_dataset = ParanmtDataset(\n",
    "    path=DATASET_PATH,\n",
    "    max_sent_size=MAX_SENT_SIZE,\n",
    "    train=True,\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3545056a-cc41-46b1-8246-1b7516cb8d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.build_vocab(\n",
    "    min_freq=2,\n",
    "    specials=['<unk>', '<pad>', '<sos>', '<eos>'],\n",
    "    max_tokens=MAX_TOKENS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67722bfa-d4d6-499a-9862-d7e9ca0ad165",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_vocab = train_dataset.toxic_vocab\n",
    "dec_vocab = train_dataset.neutral_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1e4b03a-911c-4999-8372-2e2cdfda5911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of encoder vocab: 10000\n",
      "size of decoder vocab: 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"size of encoder vocab:\", len(enc_vocab))\n",
    "print(\"size of decoder vocab:\", len(dec_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40e15519-eb8f-4164-82a4-6fe43237e8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = ParanmtDataset(\n",
    "    path=DATASET_PATH,\n",
    "    max_sent_size=MAX_SENT_SIZE,\n",
    "    vocabs=(enc_vocab, dec_vocab), # avoid data leakage\n",
    "    train=False,\n",
    "    seed=42,\n",
    "    take_first=10_000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29e54778-c844-4370-9329-93a26270d0fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(470052, 10000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa20304-6d4f-457e-bb38-518308c38543",
   "metadata": {},
   "source": [
    "## Let's create Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec52e669-ab26-42a5-a192-e0b7d46cb82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52e6306f-5a09-4ae1-a202-208fea6562bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e60c1f04-f350-4f9b-8960-1cca38e70a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic_sent.shape: torch.Size([128, 32])\n",
      "neutral_sent.shape: torch.Size([128, 32])\n"
     ]
    }
   ],
   "source": [
    "# let's check if shape and everything is ok\n",
    "for batch in train_dataloader:\n",
    "    toxic_sent, neutral_sent = batch\n",
    "    print(\"toxic_sent.shape:\", toxic_sent.shape)\n",
    "    print(\"neutral_sent.shape:\", neutral_sent.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dca808e2-529f-4c8e-b035-3b03ec7d053d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc46dfbe-d834-48e7-ad09-c49c2a676386",
   "metadata": {},
   "source": [
    "# Load the Model\n",
    "\n",
    "- Transformer architerture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c1113a1-d18c-43f3-94b1-1d44f02ac03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.transformer.encoder import Encoder\n",
    "from src.models.transformer.decoder import Decoder\n",
    "from src.models.transformer import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "451a159a-dae5-487c-be50-c87b7407add1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure some parameters for the model\n",
    "heads = 4\n",
    "hidden_dim = 256\n",
    "ff_expantion = 4\n",
    "max_size = MAX_SENT_SIZE\n",
    "\n",
    "## Encoder\n",
    "enc_input_dim = len(enc_vocab)\n",
    "enc_dropout = 0.1\n",
    "enc_num_layers = 3\n",
    "enc_padding_idx = enc_vocab['<pad>']\n",
    "\n",
    "## Decoder\n",
    "dec_output_dim = len(dec_vocab)\n",
    "dec_dropout = 0.1\n",
    "dec_num_layers = 3\n",
    "dec_padding_idx = dec_vocab['<pad>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b48dfdf-ba48-477c-ac62-2c512a69d86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "# load the encoder and decoder for our model\n",
    "encoder = Encoder(\n",
    "    input_dim=enc_input_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_layers=enc_num_layers,\n",
    "    heads=heads,\n",
    "    ff_expantion=ff_expantion,\n",
    "    dropout=enc_dropout,\n",
    "    device=device,\n",
    "    max_size=max_size,\n",
    "    vocab=enc_vocab,\n",
    ").to(device)\n",
    "\n",
    "decoder = Decoder(\n",
    "    output_dim=dec_output_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_layers=dec_num_layers,\n",
    "    heads=heads,\n",
    "    ff_expantion=ff_expantion,\n",
    "    dropout=dec_dropout,\n",
    "    device=device,\n",
    "    max_size=max_size,\n",
    "    vocab=dec_vocab,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb68fa7c-f8ee-4667-9b20-a638de4fe311",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = float('inf')\n",
    "\n",
    "model = Transformer(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    device=device,\n",
    "    max_sent_size=MAX_SENT_SIZE,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "088f446e-d95e-4812-949d-a3a5eb20ab6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=decoder.padding_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae355174-b99f-403c-b926-def9bd694c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 1: 100%|██████████| 3673/3673 [03:17<00:00, 18.63it/s, loss=4.28]\n",
      "Evaluating 1: 100%|██████████| 79/79 [00:01<00:00, 55.53it/s, loss=3.62]\n",
      "Training 2: 100%|██████████| 3673/3673 [03:17<00:00, 18.59it/s, loss=3.53]\n",
      "Evaluating 2: 100%|██████████| 79/79 [00:01<00:00, 56.64it/s, loss=3.28]\n",
      "Training 3: 100%|██████████| 3673/3673 [03:16<00:00, 18.67it/s, loss=3.27]\n",
      "Evaluating 3: 100%|██████████| 79/79 [00:01<00:00, 55.58it/s, loss=3.07]\n",
      "Training 4: 100%|██████████| 3673/3673 [03:16<00:00, 18.71it/s, loss=3.09]\n",
      "Evaluating 4: 100%|██████████| 79/79 [00:01<00:00, 55.78it/s, loss=2.91]\n",
      "Training 5: 100%|██████████| 3673/3673 [03:16<00:00, 18.74it/s, loss=2.96]\n",
      "Evaluating 5: 100%|██████████| 79/79 [00:01<00:00, 55.41it/s, loss=2.8] \n",
      "Training 6: 100%|██████████| 3673/3673 [03:16<00:00, 18.67it/s, loss=2.85]\n",
      "Evaluating 6: 100%|██████████| 79/79 [00:01<00:00, 56.60it/s, loss=2.7] \n",
      "Training 7: 100%|██████████| 3673/3673 [03:17<00:00, 18.64it/s, loss=2.76]\n",
      "Evaluating 7: 100%|██████████| 79/79 [00:01<00:00, 55.83it/s, loss=2.62]\n",
      "Training 8: 100%|██████████| 3673/3673 [03:16<00:00, 18.65it/s, loss=2.68]\n",
      "Evaluating 8: 100%|██████████| 79/79 [00:01<00:00, 57.10it/s, loss=2.56]\n",
      "Training 9: 100%|██████████| 3673/3673 [03:16<00:00, 18.65it/s, loss=2.61]\n",
      "Evaluating 9: 100%|██████████| 79/79 [00:01<00:00, 57.25it/s, loss=2.49]\n",
      "Training 10: 100%|██████████| 3673/3673 [03:16<00:00, 18.70it/s, loss=2.55]\n",
      "Evaluating 10: 100%|██████████| 79/79 [00:01<00:00, 55.94it/s, loss=2.44]\n",
      "Training 11: 100%|██████████| 3673/3673 [03:16<00:00, 18.72it/s, loss=2.49]\n",
      "Evaluating 11: 100%|██████████| 79/79 [00:01<00:00, 55.78it/s, loss=2.39]\n",
      "Training 12: 100%|██████████| 3673/3673 [03:16<00:00, 18.72it/s, loss=2.44]\n",
      "Evaluating 12: 100%|██████████| 79/79 [00:01<00:00, 55.23it/s, loss=2.36]\n",
      "Training 13: 100%|██████████| 3673/3673 [03:16<00:00, 18.71it/s, loss=2.4]\n",
      "Evaluating 13: 100%|██████████| 79/79 [00:01<00:00, 56.14it/s, loss=2.31]\n",
      "Training 14: 100%|██████████| 3673/3673 [03:16<00:00, 18.71it/s, loss=2.36]\n",
      "Evaluating 14: 100%|██████████| 79/79 [00:01<00:00, 56.95it/s, loss=2.29]\n",
      "Training 15: 100%|██████████| 3673/3673 [03:15<00:00, 18.76it/s, loss=2.32]\n",
      "Evaluating 15: 100%|██████████| 79/79 [00:01<00:00, 55.95it/s, loss=2.26]\n",
      "Training 16: 100%|██████████| 3673/3673 [03:16<00:00, 18.73it/s, loss=2.29]\n",
      "Evaluating 16: 100%|██████████| 79/79 [00:01<00:00, 56.97it/s, loss=2.23]\n",
      "Training 17: 100%|██████████| 3673/3673 [03:15<00:00, 18.74it/s, loss=2.26]\n",
      "Evaluating 17: 100%|██████████| 79/79 [00:01<00:00, 56.00it/s, loss=2.21]\n",
      "Training 18:  38%|███▊      | 1406/3673 [01:15<02:00, 18.88it/s, loss=2.23]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Training 19: 100%|██████████| 3673/3673 [03:16<00:00, 18.71it/s, loss=2.2]\n",
      "Evaluating 19: 100%|██████████| 79/79 [00:01<00:00, 54.80it/s, loss=2.18]\n",
      "Training 20: 100%|██████████| 3673/3673 [03:16<00:00, 18.69it/s, loss=2.18]\n",
      "Evaluating 20: 100%|██████████| 79/79 [00:01<00:00, 55.68it/s, loss=2.16]\n"
     ]
    }
   ],
   "source": [
    "from src.models.train_model import train\n",
    "\n",
    "best_loss = train(\n",
    "    model=model,\n",
    "    loaders=(train_dataloader, val_dataloader),\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    epochs=20,\n",
    "    device=device,\n",
    "    best_loss=best_loss,\n",
    "    ckpt_path=MODEL_CHECKPOINT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7dc1b6ad-901d-4582-84c8-79ba34fdcebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's load the model and predict\n",
    "model = torch.load(MODEL_CHECKPOINT)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3f96417-66e1-4061-ba49-55207e953142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic_sent: you earn a living by torturing people.\n",
      "neutral_sent: you torture people for a living.\n",
      "predictions:\n",
      "\t1) ['<sos>', 'you', 'are', '<unk>', 'by', 'torturing', 'people', '.', '<eos>']\n",
      "\t2) ['<sos>', 'you', 'are', 'afraid', 'of', 'being', 'tortured', 'people', '.', '<eos>']\n",
      "\t3) ['<sos>', 'you', 'earn', 'a', 'living', 'person', '.', '<eos>']\n",
      "\n",
      "\n",
      "toxic_sent: i do not give a damn about his education.\n",
      "neutral_sent: i do not care about his education.\n",
      "predictions:\n",
      "\t1) ['<sos>', 'i', 'do', 'not', 'care', 'about', 'his', 'education', '.', '<eos>']\n",
      "\t2) ['<sos>', 'i', 'do', 'not', 'care', 'about', 'his', 'education', 'education', '.', '<eos>']\n",
      "\t3) ['<sos>', 'i', 'do', 'not', 'care', 'about', 'the', 'education', '.', '<eos>']\n",
      "\n",
      "\n",
      "toxic_sent: why did not you have sex with her?\n",
      "neutral_sent: why did not you give her one?\n",
      "predictions:\n",
      "\t1) ['<sos>', 'why', 'did', 'not', 'you', 'sleep', 'with', 'her', '?', '<eos>']\n",
      "\t2) ['<sos>', 'why', 'did', 'not', 'you', 'sleep', 'with', 'it', '?', '<eos>']\n",
      "\t3) ['<sos>', 'why', 'did', 'not', 'you', 'have', 'sex', 'with', 'her', '?', '<eos>']\n",
      "\n",
      "\n",
      "toxic_sent: to seal the deal, they killed and drank the blood of the ninth one who refused.\n",
      "neutral_sent: to seal the contract, they killed what he refused and turned off his blood.\n",
      "predictions:\n",
      "\t1) ['<sos>', 'to', 'seal', 'the', 'seal', ',', 'and', 'they', 'drank', 'the', 'ninth', 'of', 'the', 'blood', 'who', 'refused', '.', '<eos>']\n",
      "\t2) ['<sos>', 'to', 'seal', 'the', 'seal', ',', 'and', 'drank', 'the', 'ninth', 'of', 'the', 'blood', 'who', 'refused', '.', '<eos>']\n",
      "\t3) ['<sos>', 'to', 'seal', 'the', 'seal', ',', 'they', 'drank', 'the', 'ninth', 'and', 'drank', 'the', 'blood', 'of', 'the', 'blood', 'who', 'refused', '.', '<eos>']\n",
      "\n",
      "\n",
      "toxic_sent: well, that is funny, because your brother throws like a girl.\n",
      "neutral_sent: that is ginger, that is your brother nazi as a girl.\n",
      "predictions:\n",
      "\t1) ['<sos>', 'well', ',', 'that', 'is', 'funny', 'because', 'your', 'brother', 'like', 'a', 'girl', '.', '<eos>']\n",
      "\t2) ['<sos>', 'well', ',', 'that', 'is', 'funny', ',', 'because', 'your', 'brother', 'like', 'a', 'girl', '.', '<eos>']\n",
      "\t3) ['<sos>', 'that', 'is', 'funny', 'because', 'your', 'brother', 'like', 'a', 'girl', '.', '<eos>']\n",
      "\n",
      "\n",
      "toxic_sent: we are going to cut you up, but you are not going to sue us!\n",
      "neutral_sent: we will chop, but you are not going to sue!\n",
      "predictions:\n",
      "\t1) ['<sos>', 'we', 'are', 'going', 'to', 'cut', 'you', ',', 'but', 'you', 'are', 'not', 'going', 'to', 'sue', 'us', '!', '<eos>']\n",
      "\t2) ['<sos>', 'we', 'will', 'cut', 'you', ',', 'but', 'you', 'are', 'not', 'going', 'to', 'sue', 'us', '!', '<eos>']\n",
      "\t3) ['<sos>', 'we', 'are', 'going', 'to', 'cut', 'you', ',', 'but', 'you', 'will', 'not', 'sue', 'us', '!', '<eos>']\n",
      "\n",
      "\n",
      "toxic_sent: charlie shut up.\n",
      "neutral_sent: silence from charlie.\n",
      "predictions:\n",
      "\t1) ['<sos>', 'charlie', 'quiet', '.', '<eos>']\n",
      "\t2) ['<sos>', 'charlie', 'be', 'quiet', '.', '<eos>']\n",
      "\t3) ['<sos>', 'charlie', 'shut', 'up', '.', '<eos>']\n",
      "\n",
      "\n",
      "toxic_sent: idol worshippers!\n",
      "neutral_sent: these believers!\n",
      "predictions:\n",
      "\t1) ['<sos>', '<unk>', '!', '<eos>']\n",
      "\t2) ['<sos>', '<unk>', '<unk>', '!', '<eos>']\n",
      "\t3) ['<sos>', 'slave', '<unk>', '!', '<eos>']\n",
      "\n",
      "\n",
      "toxic_sent: you must be happy the old folk are dead like you wanted\n",
      "neutral_sent: I am sure you are happy that the old speakers are finally dead as you wanted.\n",
      "predictions:\n",
      "\t1) ['<sos>', 'you', 'must', 'be', 'happy', 'to', 'be', 'the', 'old', 'man', 'like', 'you', 'wanted', '.', '<eos>']\n",
      "\t2) ['<sos>', 'you', 'must', 'be', 'happy', 'when', 'you', 'are', 'dead', '.', '<eos>']\n",
      "\t3) ['<sos>', 'you', 'must', 'be', 'happy', 'when', 'you', 'were', 'dead', '.', '<eos>']\n",
      "\n",
      "\n",
      "toxic_sent: first time in my life, i do the right thing . - shut up.\n",
      "neutral_sent: for the first time in my life, I have done the right thing.\n",
      "predictions:\n",
      "\t1) ['<sos>', 'first', 'time', 'in', 'my', 'life', ',', 'i', 'do', 'not', 'shut', 'up', '.', '<eos>']\n",
      "\t2) ['<sos>', 'first', 'time', 'in', 'my', 'life', ',', 'i', 'do', 'the', 'right', '.', '<eos>']\n",
      "\t3) ['<sos>', 'first', 'time', 'in', 'my', 'life', ',', 'i', 'do', 'not', 'shut', 'it', 'up', '.', '<eos>']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "detokenizer = TreebankWordDetokenizer()\n",
    "\n",
    "# let's see how our model works\n",
    "num_examples = 10\n",
    "num_sentence = 3\n",
    "dataset = val_dataset\n",
    "for idx in range(num_examples):\n",
    "    idx = np.random.randint(0, len(dataset))\n",
    "    toxic_sent = detokenizer.detokenize(dataset.df.loc[idx, 'toxic_sent'])\n",
    "    neutral_sent = detokenizer.detokenize(dataset.df.loc[idx, 'neutral_sent'])\n",
    "    \n",
    "    print('toxic_sent:', toxic_sent)\n",
    "    print('neutral_sent:', neutral_sent)\n",
    "    \n",
    "    # let's use beam search\n",
    "    # i turned off postprocess_text on purpose \n",
    "    # to see everything (postprocess_text removes some tokens and detokenize the sentence)\n",
    "    preds = model.predict(\n",
    "        toxic_sent,\n",
    "        use_beam_search=True,\n",
    "        num_candidates=num_sentence,\n",
    "        post_process_text=False\n",
    "    )\n",
    "    print(\"predictions:\")\n",
    "    for i in range(num_sentence):\n",
    "        print(f\"\\t{i+1})\", preds[i])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a64a1c19-80aa-40fc-bf7d-6b55d819fbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_bleu(dataset, model):\n",
    "    preds = []\n",
    "    trgs = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(len(dataset))):\n",
    "            toxic_sent, neutral_sent = dataset[i]\n",
    "            toxic_sent = toxic_sent.to(model.device).unsqueeze(0)\n",
    "            pred = model.predict(toxic_sent, post_process_text=False)\n",
    "            \n",
    "            pred = pred[1:-1] # remove <sos> and <eos>\n",
    "            \n",
    "            neutral_sent = model.decoder.vocab.lookup_tokens(neutral_sent.numpy())\n",
    "            neutral_sent = neutral_sent[1:] # remove <sos>\n",
    "            neutral_sent = neutral_sent[:neutral_sent.index('<eos>')]\n",
    "            \n",
    "            preds.append(pred)\n",
    "            trgs.append([neutral_sent])\n",
    "        \n",
    "    return bleu_score(preds, trgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "518068d1-2d67-4c58-8e33-f577b0a53cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [04:18<00:00, 38.70it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2538011075535255"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_bleu(val_dataset, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e41c9e-be99-4275-bee1-ebc699428aea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
