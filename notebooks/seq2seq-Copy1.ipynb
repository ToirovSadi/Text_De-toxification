{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51b0681a-69e2-4f9f-9ed4-01c96556980d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_MODEL = 'models/seq2seq.01.pt'\n",
    "MODEL_CHECKPOINT = 'models/seq2seq.01.pt'\n",
    "DATASET_PATH = 'data/interim/preprocessed_paranmt.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40068414-5b9f-4586-bcdb-470c3efb7c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os\n",
    "os.chdir(\"..\") # go to the root dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd03cca-29af-4e00-865b-e0c255cc787f",
   "metadata": {},
   "source": [
    "# Get the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4a86d33-7970-403b-b0e7-601c13db61ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENT_SIZE = 10\n",
    "MAX_TOKENS = 8_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e44e86ca-4ca7-4594-9ce7-3dc4c457cf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.make_dataset import ParanmtDataset\n",
    "\n",
    "train_dataset = ParanmtDataset(\n",
    "    path=DATASET_PATH,\n",
    "    max_sent_size=MAX_SENT_SIZE,\n",
    "    train=True,\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3822e15e-e711-43c1-a0c2-5e912b406de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.build_vocab(\n",
    "    min_freq=2,\n",
    "    specials=['<unk>', '<pad>', '<sos>', '<eos>'],\n",
    "    max_tokens=MAX_TOKENS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "546d87b2-3b10-4951-ac9e-fa936f2ef204",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_vocab = train_dataset.toxic_vocab\n",
    "dec_vocab = train_dataset.neutral_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc99d633-b8bc-4ae0-b56d-78ec8642c42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of encoder vocab: 8000\n",
      "size of decoder vocab: 8000\n"
     ]
    }
   ],
   "source": [
    "print(\"size of encoder vocab:\", len(enc_vocab))\n",
    "print(\"size of decoder vocab:\", len(dec_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a7b0469-8585-4289-9e45-5f62495f27d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = ParanmtDataset(\n",
    "    path=DATASET_PATH,\n",
    "    max_sent_size=MAX_SENT_SIZE,\n",
    "    vocabs=(enc_vocab, dec_vocab), # avoid data leakage\n",
    "    train=False,\n",
    "    seed=42,\n",
    "    take_first=10_000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7abeadd3-8c26-4b70-987b-be5e16ebdf6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>similarity</th>\n",
       "      <th>lenght_diff</th>\n",
       "      <th>toxic_sent</th>\n",
       "      <th>neutral_sent</th>\n",
       "      <th>toxic_val</th>\n",
       "      <th>neutral_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.708038</td>\n",
       "      <td>0.171429</td>\n",
       "      <td>[what, the, hell, i, danger, looking, at, ?]</td>\n",
       "      <td>[what, the, hell, is, safe, watch, ?]</td>\n",
       "      <td>0.888703</td>\n",
       "      <td>0.130954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.606822</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>[lisa, ,, hit, him, again, .]</td>\n",
       "      <td>[lisa, ,, one, more, .]</td>\n",
       "      <td>0.957538</td>\n",
       "      <td>0.000053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.719271</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>[what, are, you, doing, with, that, hooker, ?]</td>\n",
       "      <td>[what, are, you, doing, with, that, outsider, ?]</td>\n",
       "      <td>0.998877</td>\n",
       "      <td>0.000056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.821008</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>[we, are, going, to, hit, him, !]</td>\n",
       "      <td>[it, is, going, to, hit, !]</td>\n",
       "      <td>0.997299</td>\n",
       "      <td>0.014387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.725030</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>[i, do, not, fucking, believe, it, !]</td>\n",
       "      <td>[i, do, not, freaking, believe, it]</td>\n",
       "      <td>0.957814</td>\n",
       "      <td>0.056393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157735</th>\n",
       "      <td>0.827812</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>[I, will, make, you, fall, !]</td>\n",
       "      <td>[I, am, going, to, fall, !]</td>\n",
       "      <td>0.590488</td>\n",
       "      <td>0.006672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157736</th>\n",
       "      <td>0.625040</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>[i, fucking, my, girlfriend, .]</td>\n",
       "      <td>[satisfying, my, girl, behind, my, back, .]</td>\n",
       "      <td>0.999578</td>\n",
       "      <td>0.029578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157737</th>\n",
       "      <td>0.815115</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>[he, is, going, to, shoot, again, .]</td>\n",
       "      <td>[he, is, going, to, fire, again, .]</td>\n",
       "      <td>0.989201</td>\n",
       "      <td>0.008294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157738</th>\n",
       "      <td>0.866068</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>[oh, ,, mars, solid, ,, you, stink, .]</td>\n",
       "      <td>[oh, ,, mars, solid, ,, you, smell, .]</td>\n",
       "      <td>0.999077</td>\n",
       "      <td>0.072257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157739</th>\n",
       "      <td>0.870897</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>[you, going, to, beat, up, everybody, ?]</td>\n",
       "      <td>[you, going, to, beat, them, all, ?]</td>\n",
       "      <td>0.978554</td>\n",
       "      <td>0.018835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>157740 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        similarity  lenght_diff  \\\n",
       "0         0.708038     0.171429   \n",
       "1         0.606822     0.238095   \n",
       "2         0.719271     0.051282   \n",
       "3         0.821008     0.047619   \n",
       "4         0.725030     0.096774   \n",
       "...            ...          ...   \n",
       "157735    0.827812     0.200000   \n",
       "157736    0.625040     0.333333   \n",
       "157737    0.815115     0.041667   \n",
       "157738    0.866068     0.037037   \n",
       "157739    0.870897     0.137931   \n",
       "\n",
       "                                            toxic_sent  \\\n",
       "0         [what, the, hell, i, danger, looking, at, ?]   \n",
       "1                        [lisa, ,, hit, him, again, .]   \n",
       "2       [what, are, you, doing, with, that, hooker, ?]   \n",
       "3                    [we, are, going, to, hit, him, !]   \n",
       "4                [i, do, not, fucking, believe, it, !]   \n",
       "...                                                ...   \n",
       "157735                   [I, will, make, you, fall, !]   \n",
       "157736                 [i, fucking, my, girlfriend, .]   \n",
       "157737            [he, is, going, to, shoot, again, .]   \n",
       "157738          [oh, ,, mars, solid, ,, you, stink, .]   \n",
       "157739        [you, going, to, beat, up, everybody, ?]   \n",
       "\n",
       "                                            neutral_sent  toxic_val  \\\n",
       "0                  [what, the, hell, is, safe, watch, ?]   0.888703   \n",
       "1                                [lisa, ,, one, more, .]   0.957538   \n",
       "2       [what, are, you, doing, with, that, outsider, ?]   0.998877   \n",
       "3                            [it, is, going, to, hit, !]   0.997299   \n",
       "4                    [i, do, not, freaking, believe, it]   0.957814   \n",
       "...                                                  ...        ...   \n",
       "157735                       [I, am, going, to, fall, !]   0.590488   \n",
       "157736       [satisfying, my, girl, behind, my, back, .]   0.999578   \n",
       "157737               [he, is, going, to, fire, again, .]   0.989201   \n",
       "157738            [oh, ,, mars, solid, ,, you, smell, .]   0.999077   \n",
       "157739              [you, going, to, beat, them, all, ?]   0.978554   \n",
       "\n",
       "        neutral_val  \n",
       "0          0.130954  \n",
       "1          0.000053  \n",
       "2          0.000056  \n",
       "3          0.014387  \n",
       "4          0.056393  \n",
       "...             ...  \n",
       "157735     0.006672  \n",
       "157736     0.029578  \n",
       "157737     0.008294  \n",
       "157738     0.072257  \n",
       "157739     0.018835  \n",
       "\n",
       "[157740 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1688b81e-8e3c-4266-8ab9-07f52124e827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(157740, 10000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd8b060-2108-4d18-865c-f9de2ae7969c",
   "metadata": {},
   "source": [
    "# Build the Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f82c34f-8f03-45ae-8eae-02dd2759bf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7d5e2e4-dce6-470f-9f5d-06a09c798d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7953a61f-64c0-49a3-9fda-f64e80861fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic_sent.shape: torch.Size([128, 10])\n",
      "neutral_sent.shape: torch.Size([128, 10])\n"
     ]
    }
   ],
   "source": [
    "# let's check if shape and everything is ok\n",
    "for batch in train_dataloader:\n",
    "    toxic_sent, neutral_sent = batch\n",
    "    print(\"toxic_sent.shape:\", toxic_sent.shape)\n",
    "    print(\"neutral_sent.shape:\", neutral_sent.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dca808e2-529f-4c8e-b035-3b03ec7d053d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc46dfbe-d834-48e7-ad09-c49c2a676386",
   "metadata": {},
   "source": [
    "# Load the Model\n",
    "\n",
    "- Simple EncoderDecoder (Seq2Seq) architerture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08d88aaa-7e97-411f-9164-4bdbfae09a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.seq2seq.encoder import Encoder\n",
    "from src.models.seq2seq.decoder import Decoder\n",
    "from src.models.seq2seq import Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "451a159a-dae5-487c-be50-c87b7407add1",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(enc_vocab)\n",
    "OUTPUT_DIM = len(dec_vocab)\n",
    "EMBED_DIM = 256\n",
    "NUM_HIDDEN = 512\n",
    "N_LAYERS = 6\n",
    "DROPOUT = 0.5\n",
    "ENC_PADDING_IDX = enc_vocab['<pad>']\n",
    "DEC_PADDING_IDX = dec_vocab['<pad>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b48dfdf-ba48-477c-ac62-2c512a69d86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the encoder and decoder for our model\n",
    "encoder = Encoder(\n",
    "    input_dim=INPUT_DIM,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    hidden_dim=NUM_HIDDEN,\n",
    "    num_layers=N_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    vocab=enc_vocab,\n",
    "    padding_idx=ENC_PADDING_IDX\n",
    ").to(device)\n",
    "\n",
    "decoder = Decoder(\n",
    "    output_dim=OUTPUT_DIM,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    hidden_dim=NUM_HIDDEN,\n",
    "    num_layers=N_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    vocab=dec_vocab,\n",
    "    padding_idx=DEC_PADDING_IDX\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb68fa7c-f8ee-4667-9b20-a638de4fe311",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = float('inf')\n",
    "\n",
    "model = Seq2Seq(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    device=device,\n",
    "    max_sent_size=MAX_SENT_SIZE,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c0d4feb-d171-4c6d-99dd-d05fb9e804c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters in model: 32.0M\n"
     ]
    }
   ],
   "source": [
    "from src.models.utils import count_parameters\n",
    "\n",
    "print(f\"number of parameters in model: {count_parameters(model)//1e6}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "088f446e-d95e-4812-949d-a3a5eb20ab6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=dec_vocab['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae355174-b99f-403c-b926-def9bd694c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1233/1233 [01:09<00:00, 17.63it/s, loss=4.57]\n",
      "Evaluating 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 58.37it/s, loss=4.34]\n",
      "Training 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1233/1233 [01:09<00:00, 17.69it/s, loss=3.68]\n",
      "Evaluating 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 58.08it/s, loss=4.04]\n",
      "Training 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1233/1233 [01:09<00:00, 17.65it/s, loss=3.28]\n",
      "Evaluating 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 57.33it/s, loss=3.95]\n",
      "Training 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1233/1233 [01:09<00:00, 17.64it/s, loss=3.06]\n",
      "Evaluating 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 58.74it/s, loss=3.91]\n",
      "Training 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1233/1233 [01:09<00:00, 17.66it/s, loss=2.9] \n",
      "Evaluating 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 56.57it/s, loss=3.84]\n",
      "Training 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1233/1233 [01:10<00:00, 17.60it/s, loss=2.77]\n",
      "Evaluating 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 58.56it/s, loss=3.81]\n",
      "Training 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1233/1233 [01:09<00:00, 17.65it/s, loss=2.66]\n",
      "Evaluating 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 58.56it/s, loss=3.79]\n",
      "Training 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1233/1233 [01:09<00:00, 17.65it/s, loss=2.57]\n",
      "Evaluating 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 58.07it/s, loss=3.77]\n",
      "Training 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1233/1233 [01:09<00:00, 17.62it/s, loss=2.48]\n",
      "Evaluating 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 58.09it/s, loss=3.77]\n",
      "Training 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1233/1233 [01:09<00:00, 17.65it/s, loss=2.4] \n",
      "Evaluating 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 55.06it/s, loss=3.8] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update teacher force to 0.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1233/1233 [01:10<00:00, 17.57it/s, loss=2.34]\n",
      "Evaluating 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 57.33it/s, loss=3.77]\n",
      "Training 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1233/1233 [01:09<00:00, 17.64it/s, loss=2.27]\n",
      "Evaluating 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 57.56it/s, loss=3.81]\n",
      "Training 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1233/1233 [01:09<00:00, 17.64it/s, loss=2.22]\n",
      "Evaluating 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 56.15it/s, loss=3.84]\n",
      "Training 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1233/1233 [01:09<00:00, 17.69it/s, loss=2.17]\n",
      "Evaluating 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 57.70it/s, loss=3.82]\n",
      "Training 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1233/1233 [01:10<00:00, 17.60it/s, loss=2.13]\n",
      "Evaluating 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 57.62it/s, loss=3.86]\n",
      "Training 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1233/1233 [01:10<00:00, 17.60it/s, loss=2.08]\n",
      "Evaluating 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 56.44it/s, loss=3.87]\n",
      "Training 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1233/1233 [01:10<00:00, 17.60it/s, loss=2.04]\n",
      "Evaluating 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 57.55it/s, loss=3.86]\n",
      "Training 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1233/1233 [01:09<00:00, 17.65it/s, loss=2]   \n",
      "Evaluating 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 57.95it/s, loss=3.83]\n",
      "Training 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1233/1233 [01:09<00:00, 17.67it/s, loss=1.97]\n",
      "Evaluating 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 57.53it/s, loss=3.88]\n",
      "Training 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1233/1233 [01:09<00:00, 17.63it/s, loss=1.93]\n",
      "Evaluating 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:01<00:00, 56.59it/s, loss=3.91]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update teacher force to 0.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from src.models.train_model import train\n",
    "\n",
    "best_loss = train(\n",
    "    model=model,\n",
    "    loaders=(train_dataloader, val_dataloader),\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    epochs=20,\n",
    "    device=device,\n",
    "    best_loss=best_loss,\n",
    "    ckpt_path=MODEL_CHECKPOINT,\n",
    "    clip_grad=1,\n",
    "    teacher_force={\n",
    "        'value': 0.85,\n",
    "        'gamma': 1.0,\n",
    "        'update_every_n_epoch': 10,\n",
    "    } # first 10 epoch teacher force 1, after it will be turned off\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7dc1b6ad-901d-4582-84c8-79ba34fdcebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (vocab): Vocab()\n",
       "    (embedding): Embedding(8000, 256, padding_idx=1)\n",
       "    (rnn): LSTM(256, 512, num_layers=6, batch_first=True, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (vocab): Vocab()\n",
       "    (embedding): Embedding(8000, 256, padding_idx=1)\n",
       "    (rnn): LSTM(256, 512, num_layers=6, batch_first=True, dropout=0.5)\n",
       "    (fc_out): Linear(in_features=512, out_features=8000, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's load the model and predict\n",
    "model = torch.load(MODEL_CHECKPOINT)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3f96417-66e1-4061-ba49-55207e953142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic_sent: let me tell the damn story now!\n",
      "neutral_sent: I am telling this story!\n",
      "predictions:\n",
      "\t1) ['give', 'me', 'the', 'keys', '!', '<eos>']\n",
      "\t2) ['give', 'me', 'the', 'money', '!', '<eos>']\n",
      "\t3) ['give', 'me', 'the', 'gun', '!', '<eos>']\n",
      "\n",
      "\n",
      "toxic_sent: you fucking believe this?\n",
      "neutral_sent: do you believe that?\n",
      "predictions:\n",
      "\t1) ['do', 'you', 'believe', 'it', '?', '<eos>']\n",
      "\t2) ['can', 'you', 'believe', 'it', '?', '<eos>']\n",
      "\t3) ['do', 'you', 'believe', 'this', '?', '<eos>']\n",
      "\n",
      "\n",
      "toxic_sent: shut up, bean paste!\n",
      "neutral_sent: stop talking, bean paste!\n",
      "predictions:\n",
      "\t1) ['quiet', ',', '<unk>', '<unk>', '!', '<eos>']\n",
      "\t2) ['quiet', ',', '<unk>', '!', '<eos>']\n",
      "\t3) ['hush', ',', '<unk>', '<unk>', '!', '<eos>']\n",
      "\n",
      "\n",
      "toxic_sent: you swear to god? you crazy?\n",
      "neutral_sent: you swear by god?\n",
      "predictions:\n",
      "\t1) ['are', 'you', 'kidding', 'me', '?', '<eos>']\n",
      "\t2) ['you', 'are', 'kidding', 'me', '?', '<eos>']\n",
      "\t3) ['are', 'you', 'kidding', 'me', 'too', '?', '<eos>']\n",
      "\n",
      "\n",
      "toxic_sent: get the hell out of here!\n",
      "neutral_sent: get out of here!\n",
      "predictions:\n",
      "\t1) ['get', 'out', 'of', 'here', '!', '<eos>']\n",
      "\t2) ['get', 'out', 'of', 'here', '.', '<eos>']\n",
      "\t3) ['get', 'out', 'of', 'there', '!', '<eos>']\n",
      "\n",
      "\n",
      "toxic_sent: this guy is a little slimy.\n",
      "neutral_sent: the guy sounds a bit slippery.\n",
      "predictions:\n",
      "\t1) ['this', 'guy', 'is', 'a', 'bad', 'guy', '.', '<eos>']\n",
      "\t2) ['this', 'guy', 'is', 'a', '<unk>', '.', '<eos>']\n",
      "\t3) ['this', 'guy', 'is', 'a', '<unk>', '.', '.', '<eos>']\n",
      "\n",
      "\n",
      "toxic_sent: you son of a bitch.\n",
      "neutral_sent: you dog, you.\n",
      "predictions:\n",
      "\t1) ['you', 'little', 'punk', '.', '<eos>']\n",
      "\t2) ['you', 'son', 'of', 'a', '<eos>']\n",
      "\t3) ['you', 'little', 'punk', '!', '<eos>']\n",
      "\n",
      "\n",
      "toxic_sent: shut up whining.\n",
      "neutral_sent: stop crying.\n",
      "predictions:\n",
      "\t1) ['be', 'quiet', '.', '<eos>']\n",
      "\t2) ['stop', 'talking', '.', '<eos>']\n",
      "\t3) ['stop', 'quiet', '.', '<eos>']\n",
      "\n",
      "\n",
      "toxic_sent: the hell you say! exclaimed knell.\n",
      "neutral_sent: hell, no! exclaimed knell.\n",
      "predictions:\n",
      "\t1) ['you', 'know', 'what', 'you', 'know', '.', '<eos>']\n",
      "\t2) ['you', 'know', 'what', 'you', 'say', '.', '<eos>']\n",
      "\t3) ['you', 'know', 'you', 'know', 'anything', '.', '<eos>']\n",
      "\n",
      "\n",
      "toxic_sent: tasteful.\n",
      "neutral_sent: oh, elegant.\n",
      "predictions:\n",
      "\t1) ['<unk>', '.', '<eos>']\n",
      "\t2) ['the', '<unk>', '.', '<eos>']\n",
      "\t3) ['the', '.', '<eos>']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "detokenizer = TreebankWordDetokenizer()\n",
    "\n",
    "# let's see how our model works\n",
    "num_examples = 10\n",
    "num_sentence = 3\n",
    "dataset = val_dataset\n",
    "for idx in range(num_examples):\n",
    "    idx = np.random.randint(0, len(dataset))\n",
    "    toxic_sent = detokenizer.detokenize(dataset.df.loc[idx, 'toxic_sent'])\n",
    "    neutral_sent = detokenizer.detokenize(dataset.df.loc[idx, 'neutral_sent'])\n",
    "    \n",
    "    print('toxic_sent:', toxic_sent)\n",
    "    print('neutral_sent:', neutral_sent)\n",
    "    \n",
    "    # let's use beam search\n",
    "    # i turned off postprocess_text on purpose \n",
    "    # to see everything (postprocess_text removes some tokens and detokenize the sentence)\n",
    "    preds = model.predict(\n",
    "        toxic_sent,\n",
    "        beam=True,\n",
    "        beam_search_num_candidates=num_sentence,\n",
    "        post_process_text=False\n",
    "    )\n",
    "    print(\"predictions:\")\n",
    "    for i in range(num_sentence):\n",
    "        print(f\"\\t{i+1})\", preds[i])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18668afd-374c-49c2-aae3-2a65c86c0ccd",
   "metadata": {},
   "source": [
    "## Beam Search vs Greedy Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0757595-31f3-4c2b-96a3-ff32c3e1a7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic_sent: you scream like a hen.\n",
      "neutral_sent: you sound like you are howling.\n",
      "Beam Search predictions:\n",
      "\t1) ['you', 'smell', 'like', 'a', '<unk>', '.', '<eos>']\n",
      "\t2) ['you', 'smell', 'like', 'cattle', '.', '<eos>']\n",
      "\t3) ['you', 'smell', 'like', 'a', 'girl', '.', '<eos>']\n",
      "\n",
      "\n",
      "Greedy Search prediction:\n",
      "['you', 'smell', 'like', 'a', '<unk>', '.']\n",
      "\n",
      "\n",
      "toxic_sent: my mother is on die.\n",
      "neutral_sent: my mom i dying.\n",
      "Beam Search predictions:\n",
      "\t1) ['my', 'father', 'is', 'dying', '.', '<eos>']\n",
      "\t2) ['my', 'wife', 'is', 'dying', '.', '<eos>']\n",
      "\t3) ['my', 'mother', 'is', 'dying', '.', '<eos>']\n",
      "\n",
      "\n",
      "Greedy Search prediction:\n",
      "['my', 'father', 'is', 'dying', '.']\n",
      "\n",
      "\n",
      "toxic_sent: shut your mouth!\n",
      "neutral_sent: close your mouth.\n",
      "Beam Search predictions:\n",
      "\t1) ['close', 'your', 'mouth', '!', '<eos>']\n",
      "\t2) ['keep', 'your', 'mouth', '!', '<eos>']\n",
      "\t3) ['open', 'your', 'mouth', '!', '<eos>']\n",
      "\n",
      "\n",
      "Greedy Search prediction:\n",
      "['close', 'your', 'mouth', '!']\n",
      "\n",
      "\n",
      "toxic_sent: she makin' you crazy?\n",
      "neutral_sent: is he driving you crazy?\n",
      "Beam Search predictions:\n",
      "\t1) ['is', 'that', 'crazy', '?', '<eos>']\n",
      "\t2) ['is', 'he', 'crazy', '?', '<eos>']\n",
      "\t3) ['is', 'it', 'crazy', '?', '<eos>']\n",
      "\n",
      "\n",
      "Greedy Search prediction:\n",
      "['is', 'that', 'crazy', '?']\n",
      "\n",
      "\n",
      "toxic_sent: i will not sleep with an indian.\n",
      "neutral_sent: I am not sleeping with the indians.\n",
      "Beam Search predictions:\n",
      "\t1) ['I', 'am', 'not', 'sleeping', 'in', 'the', '<unk>', '.', '<eos>']\n",
      "\t2) ['I', 'am', 'not', 'sleeping', 'with', 'the', '<unk>', '.', '<eos>']\n",
      "\t3) ['I', 'am', 'not', 'interested', 'in', 'the', '<unk>', '.', '<eos>']\n",
      "\n",
      "\n",
      "Greedy Search prediction:\n",
      "['I', 'am', 'not', 'going', 'to', 'kill', '<unk>', '.']\n",
      "\n",
      "\n",
      "toxic_sent: get your fucking hands off me!\n",
      "neutral_sent: keep your hands to yourself!\n",
      "Beam Search predictions:\n",
      "\t1) ['get', 'your', 'hands', 'off', 'me', '!', '<eos>']\n",
      "\t2) ['take', 'your', 'hands', 'off', 'me', '!', '<eos>']\n",
      "\t3) ['get', 'your', 'hands', 'off', 'me', '.', '<eos>']\n",
      "\n",
      "\n",
      "Greedy Search prediction:\n",
      "['get', 'your', 'hands', 'off', 'me', '!']\n",
      "\n",
      "\n",
      "toxic_sent: ray, do something, damn it!\n",
      "neutral_sent: jesus christ, ray, do something!\n",
      "Beam Search predictions:\n",
      "\t1) ['<unk>', ',', 'for', 'god', 'i', 'sake', '!', '<eos>']\n",
      "\t2) ['<unk>', ',', 'do', 'god', 'i', 'sake', '!', '<eos>']\n",
      "\t3) ['<unk>', ',', 'do', 'god', '!', '<eos>']\n",
      "\n",
      "\n",
      "Greedy Search prediction:\n",
      "['<unk>', ',', 'do', 'god', ',', 'i', 'quiet', '!']\n",
      "\n",
      "\n",
      "toxic_sent: what a dump.\n",
      "neutral_sent: wow . still a dump.\n",
      "Beam Search predictions:\n",
      "\t1) ['what', 'a', 'mess', '.', '<eos>']\n",
      "\t2) ['what', 'a', 'douche', '.', '<eos>']\n",
      "\t3) ['what', 'a', 'cow', '.', '<eos>']\n",
      "\n",
      "\n",
      "Greedy Search prediction:\n",
      "['what', 'a', 'mess', '.']\n",
      "\n",
      "\n",
      "toxic_sent: they are clever, these nazis.\n",
      "neutral_sent: clever, these nazis.\n",
      "Beam Search predictions:\n",
      "\t1) ['they', 'are', '<unk>', ',', '<unk>', '.', '<eos>']\n",
      "\t2) ['they', 'are', 'unnatural', ',', '<unk>', '.', '<eos>']\n",
      "\t3) ['they', 'are', '<unk>', ',', '<unk>', '.', '.', '<eos>']\n",
      "\n",
      "\n",
      "Greedy Search prediction:\n",
      "['they', 'are', '<unk>', ',', '<unk>', '.']\n",
      "\n",
      "\n",
      "toxic_sent: you do not fucking tell anyone.\n",
      "neutral_sent: you do not tell anyone.\n",
      "Beam Search predictions:\n",
      "\t1) ['you', 'do', 'not', 'deserve', 'anything', '.', '<eos>']\n",
      "\t2) ['you', 'do', 'not', 'know', 'anything', '.', '<eos>']\n",
      "\t3) ['you', 'do', 'not', 'deserve', 'them', '.', '<eos>']\n",
      "\n",
      "\n",
      "Greedy Search prediction:\n",
      "['you', 'do', 'not', 'deserve', 'anything', '.']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "detokenizer = TreebankWordDetokenizer()\n",
    "\n",
    "# let's see how our model works\n",
    "num_examples = 10\n",
    "num_sentence = 3\n",
    "dataset = val_dataset\n",
    "for idx in range(num_examples):\n",
    "    idx = np.random.randint(0, len(dataset))\n",
    "    toxic_sent = detokenizer.detokenize(dataset.df.loc[idx, 'toxic_sent'])\n",
    "    neutral_sent = detokenizer.detokenize(dataset.df.loc[idx, 'neutral_sent'])\n",
    "    \n",
    "    print('toxic_sent:', toxic_sent)\n",
    "    print('neutral_sent:', neutral_sent)\n",
    "    \n",
    "    preds = model.predict(\n",
    "        toxic_sent,\n",
    "        beam=True,\n",
    "        beam_search_num_candidates=num_sentence,\n",
    "        post_process_text=False\n",
    "    )\n",
    "    print(\"Beam Search predictions:\")\n",
    "    for i in range(num_sentence):\n",
    "        print(f\"\\t{i+1})\", preds[i])\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    \n",
    "    preds = model.predict(\n",
    "        toxic_sent,\n",
    "        beam=False,\n",
    "        post_process_text=False\n",
    "    )\n",
    "    print(\"Greedy Search prediction:\")\n",
    "    print(preds)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b78cc9-6163-4b13-b9f6-80a6040a13bb",
   "metadata": {},
   "source": [
    "### Actually Greedy search doing great job"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
