{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40068414-5b9f-4586-bcdb-470c3efb7c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os\n",
    "os.chdir(\"..\") # go to the root dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd03cca-29af-4e00-865b-e0c255cc787f",
   "metadata": {},
   "source": [
    "# Get the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4a86d33-7970-403b-b0e7-601c13db61ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENT_SIZE = 12\n",
    "MAX_TOKENS = 30_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e44e86ca-4ca7-4594-9ce7-3dc4c457cf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.make_dataset import ParanmtDataset\n",
    "\n",
    "train_dataset = ParanmtDataset(\n",
    "    path='data/interim/preprocessed_paranmt.tsv',\n",
    "    max_sent_size=MAX_SENT_SIZE,\n",
    "    train=True,\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3822e15e-e711-43c1-a0c2-5e912b406de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.build_vocab(\n",
    "    min_freq=2,\n",
    "    specials=['<unk>', '<pad>', '<sos>', '<eos>'],\n",
    "    max_tokens=MAX_TOKENS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "546d87b2-3b10-4951-ac9e-fa936f2ef204",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = train_dataset.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc99d633-b8bc-4ae0-b56d-78ec8642c42f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29878"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a7b0469-8585-4289-9e45-5f62495f27d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = ParanmtDataset(\n",
    "    path='data/interim/preprocessed_paranmt.tsv',\n",
    "    max_sent_size=MAX_SENT_SIZE,\n",
    "    vocab=vocab, # avoid data leakage\n",
    "    train=False,\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7abeadd3-8c26-4b70-987b-be5e16ebdf6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>similarity</th>\n",
       "      <th>lenght_diff</th>\n",
       "      <th>toxic_sent</th>\n",
       "      <th>neutral_sent</th>\n",
       "      <th>toxic_val</th>\n",
       "      <th>neutral_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.699613</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>[they, simply, hit, the, ground, dead, .]</td>\n",
       "      <td>[they, just, died, on, the, spot, .]</td>\n",
       "      <td>0.985227</td>\n",
       "      <td>0.000696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.736382</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>[why, not, ,, what, the, hell, .]</td>\n",
       "      <td>[after, all, ,, why, not, .]</td>\n",
       "      <td>0.886357</td>\n",
       "      <td>0.000042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.716637</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>[cigarettes, and, beer, kick, ass, .]</td>\n",
       "      <td>[cigarettes, and, beer, are, great, !]</td>\n",
       "      <td>0.997185</td>\n",
       "      <td>0.000066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.623832</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>[half, of, dodds, ', breasts, disappeared, .]</td>\n",
       "      <td>[half, of, dodd, 's, chest, dissolved, .]</td>\n",
       "      <td>0.981391</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.834294</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>[flew, out, of, nigeria, ,, crashed, here, .]</td>\n",
       "      <td>[he, 's, taken, out, of, nigeria, and, crashed...</td>\n",
       "      <td>0.643518</td>\n",
       "      <td>0.001495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234587</th>\n",
       "      <td>0.943562</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>[you, have, got, to, be, fucking, kidding, me, .]</td>\n",
       "      <td>[you, have, to, be, kidding, me, .]</td>\n",
       "      <td>0.991443</td>\n",
       "      <td>0.000125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234588</th>\n",
       "      <td>0.931757</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>[it, 's, kiki, ,, the, witch, .]</td>\n",
       "      <td>[that, 's, kiki, the, witch, .]</td>\n",
       "      <td>0.976002</td>\n",
       "      <td>0.015735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234589</th>\n",
       "      <td>0.840842</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>[take, the, dog, and, hit, it, with, a, brick, .]</td>\n",
       "      <td>[grab, the, dog, to, hit, a, brick]</td>\n",
       "      <td>0.993978</td>\n",
       "      <td>0.053225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234590</th>\n",
       "      <td>0.891694</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>[we, all, got, ta, die, sometime, ,, right, ?]</td>\n",
       "      <td>[we, 're, all, gon, na, die, someday, ,, right...</td>\n",
       "      <td>0.963766</td>\n",
       "      <td>0.094380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234591</th>\n",
       "      <td>0.746781</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>[waiting, for, him, to, die, ?]</td>\n",
       "      <td>[had, they, waited, for, his, death, ?]</td>\n",
       "      <td>0.964957</td>\n",
       "      <td>0.004026</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>234592 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        similarity  lenght_diff  \\\n",
       "0         0.699613     0.151515   \n",
       "1         0.736382     0.166667   \n",
       "2         0.716637     0.032258   \n",
       "3         0.623832     0.111111   \n",
       "4         0.834294     0.227273   \n",
       "...            ...          ...   \n",
       "234587    0.943562     0.307692   \n",
       "234588    0.931757     0.043478   \n",
       "234589    0.840842     0.263158   \n",
       "234590    0.891694     0.055556   \n",
       "234591    0.746781     0.225806   \n",
       "\n",
       "                                               toxic_sent  \\\n",
       "0               [they, simply, hit, the, ground, dead, .]   \n",
       "1                       [why, not, ,, what, the, hell, .]   \n",
       "2                   [cigarettes, and, beer, kick, ass, .]   \n",
       "3           [half, of, dodds, ', breasts, disappeared, .]   \n",
       "4           [flew, out, of, nigeria, ,, crashed, here, .]   \n",
       "...                                                   ...   \n",
       "234587  [you, have, got, to, be, fucking, kidding, me, .]   \n",
       "234588                   [it, 's, kiki, ,, the, witch, .]   \n",
       "234589  [take, the, dog, and, hit, it, with, a, brick, .]   \n",
       "234590     [we, all, got, ta, die, sometime, ,, right, ?]   \n",
       "234591                    [waiting, for, him, to, die, ?]   \n",
       "\n",
       "                                             neutral_sent  toxic_val  \\\n",
       "0                    [they, just, died, on, the, spot, .]   0.985227   \n",
       "1                            [after, all, ,, why, not, .]   0.886357   \n",
       "2                  [cigarettes, and, beer, are, great, !]   0.997185   \n",
       "3               [half, of, dodd, 's, chest, dissolved, .]   0.981391   \n",
       "4       [he, 's, taken, out, of, nigeria, and, crashed...   0.643518   \n",
       "...                                                   ...        ...   \n",
       "234587                [you, have, to, be, kidding, me, .]   0.991443   \n",
       "234588                    [that, 's, kiki, the, witch, .]   0.976002   \n",
       "234589                [grab, the, dog, to, hit, a, brick]   0.993978   \n",
       "234590  [we, 're, all, gon, na, die, someday, ,, right...   0.963766   \n",
       "234591            [had, they, waited, for, his, death, ?]   0.964957   \n",
       "\n",
       "        neutral_val  \n",
       "0          0.000696  \n",
       "1          0.000042  \n",
       "2          0.000066  \n",
       "3          0.002114  \n",
       "4          0.001495  \n",
       "...             ...  \n",
       "234587     0.000125  \n",
       "234588     0.015735  \n",
       "234589     0.053225  \n",
       "234590     0.094380  \n",
       "234591     0.004026  \n",
       "\n",
       "[234592 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd8b060-2108-4d18-865c-f9de2ae7969c",
   "metadata": {},
   "source": [
    "## Build the Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f82c34f-8f03-45ae-8eae-02dd2759bf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7d5e2e4-dce6-470f-9f5d-06a09c798d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.make_dataset import collate_batch\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batch,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_batch,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7953a61f-64c0-49a3-9fda-f64e80861fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic_sent.shape: torch.Size([12, 16])\n",
      "neutral_sent.shape: torch.Size([12, 16])\n"
     ]
    }
   ],
   "source": [
    "# let's check if shape and everything is ok\n",
    "for batch in train_dataloader:\n",
    "    toxic_sent, neutral_sent = batch\n",
    "    print(\"toxic_sent.shape:\", toxic_sent.shape)\n",
    "    print(\"neutral_sent.shape:\", neutral_sent.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dca808e2-529f-4c8e-b035-3b03ec7d053d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc46dfbe-d834-48e7-ad09-c49c2a676386",
   "metadata": {},
   "source": [
    "# Load the Model\n",
    "\n",
    "- Simple EncoderDecoder (Seq2Seq) architerture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08d88aaa-7e97-411f-9164-4bdbfae09a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.seq2seq.encoder import Encoder\n",
    "from src.models.seq2seq.decoder import Decoder\n",
    "from src.models.seq2seq import Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "451a159a-dae5-487c-be50-c87b7407add1",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(vocab)\n",
    "OUTPUT_DIM = len(vocab)\n",
    "EMBED_DIM = 128\n",
    "NUM_HIDDEN = 256\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.5\n",
    "PADDING_IDX = vocab['<pad>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b48dfdf-ba48-477c-ac62-2c512a69d86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the encoder and decoder for our model\n",
    "encoder = Encoder(\n",
    "    input_dim=INPUT_DIM,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    hidden_dim=NUM_HIDDEN,\n",
    "    num_layers=N_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    padding_idx=PADDING_IDX\n",
    ").to(device)\n",
    "\n",
    "decoder = Decoder(\n",
    "    output_dim=OUTPUT_DIM,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    hidden_dim=NUM_HIDDEN,\n",
    "    num_layers=N_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    padding_idx=PADDING_IDX\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb68fa7c-f8ee-4667-9b20-a638de4fe311",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = float('inf')\n",
    "\n",
    "model = Seq2Seq(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    device=device,\n",
    "    max_sent_size=MAX_SENT_SIZE,\n",
    "    vocab=vocab,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "088f446e-d95e-4812-949d-a3a5eb20ab6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=vocab['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae355174-b99f-403c-b926-def9bd694c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 1: 100%|██████████| 13033/13033 [04:20<00:00, 49.94it/s, loss=4.07]\n",
      "Evaluating 1: 100%|██████████| 3259/3259 [00:16<00:00, 195.14it/s, loss=3.94]\n",
      "Training 2: 100%|██████████| 13033/13033 [04:19<00:00, 50.29it/s, loss=3.57]\n",
      "Evaluating 2: 100%|██████████| 3259/3259 [00:16<00:00, 192.94it/s, loss=3.81]\n",
      "Training 3: 100%|██████████| 13033/13033 [04:20<00:00, 50.09it/s, loss=3.4]\n",
      "Evaluating 3: 100%|██████████| 3259/3259 [00:16<00:00, 191.94it/s, loss=3.69]\n",
      "Training 4: 100%|██████████| 13033/13033 [04:22<00:00, 49.73it/s, loss=3.29]\n",
      "Evaluating 4: 100%|██████████| 3259/3259 [00:16<00:00, 200.23it/s, loss=3.64]\n",
      "Training 5: 100%|██████████| 13033/13033 [04:22<00:00, 49.65it/s, loss=3.21]\n",
      "Evaluating 5: 100%|██████████| 3259/3259 [00:17<00:00, 191.55it/s, loss=3.61]\n",
      "Training 6: 100%|██████████| 13033/13033 [04:21<00:00, 49.92it/s, loss=3.14]\n",
      "Evaluating 6: 100%|██████████| 3259/3259 [00:16<00:00, 195.08it/s, loss=3.58]\n",
      "Training 7: 100%|██████████| 13033/13033 [04:21<00:00, 49.80it/s, loss=3.09]\n",
      "Evaluating 7: 100%|██████████| 3259/3259 [00:16<00:00, 195.05it/s, loss=3.55]\n",
      "Training 8: 100%|██████████| 13033/13033 [04:21<00:00, 49.86it/s, loss=3.05]\n",
      "Evaluating 8: 100%|██████████| 3259/3259 [00:16<00:00, 192.22it/s, loss=3.53]\n",
      "Training 9: 100%|██████████| 13033/13033 [04:22<00:00, 49.60it/s, loss=3.01]\n",
      "Evaluating 9: 100%|██████████| 3259/3259 [00:16<00:00, 198.25it/s, loss=3.53]\n",
      "Training 10: 100%|██████████| 13033/13033 [04:21<00:00, 49.93it/s, loss=2.98]\n",
      "Evaluating 10: 100%|██████████| 3259/3259 [00:16<00:00, 195.94it/s, loss=3.51]\n",
      "Training 13: 100%|██████████| 13033/13033 [09:25<00:00, 23.03it/s, loss=2.89]\n",
      "Evaluating 13: 100%|██████████| 3259/3259 [00:47<00:00, 68.03it/s, loss=3.48]\n",
      "Training 14: 100%|██████████| 13033/13033 [08:30<00:00, 25.54it/s, loss=2.87]\n",
      "Evaluating 14: 100%|██████████| 3259/3259 [00:47<00:00, 68.40it/s, loss=3.48]\n",
      "Training 15: 100%|██████████| 13033/13033 [09:26<00:00, 23.02it/s, loss=2.84]\n",
      "Evaluating 15: 100%|██████████| 3259/3259 [00:48<00:00, 67.41it/s, loss=3.46]\n",
      "Training 16: 100%|██████████| 13033/13033 [08:30<00:00, 25.54it/s, loss=2.82]\n",
      "Evaluating 16: 100%|██████████| 3259/3259 [00:48<00:00, 67.77it/s, loss=3.45]\n",
      "Training 17: 100%|██████████| 13033/13033 [09:25<00:00, 23.06it/s, loss=2.8] \n",
      "Evaluating 17: 100%|██████████| 3259/3259 [00:48<00:00, 67.22it/s, loss=3.45]\n",
      "Training 20: 100%|██████████| 13033/13033 [08:31<00:00, 25.50it/s, loss=2.75]\n",
      "Evaluating 20: 100%|██████████| 3259/3259 [00:48<00:00, 67.37it/s, loss=3.45]\n"
     ]
    }
   ],
   "source": [
    "from src.models.train_model import train\n",
    "\n",
    "best_loss = train(\n",
    "    model=model,\n",
    "    loaders=(train_dataloader, val_dataloader),\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    epochs=20,\n",
    "    device=device,\n",
    "    best_loss=best_loss,\n",
    "    ckpt_path='models/seq2seq.pt',\n",
    "    clip_grad=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7dc1b6ad-901d-4582-84c8-79ba34fdcebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's load the model and predict\n",
    "model = torch.load('models/seq2seq.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e3f96417-66e1-4061-ba49-55207e953142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic_sent: well, i mean, honesly mostly schmucks.\n",
      "neutral_sent: well...mostly dummies.\n",
      "prediction: well, i mean, well.\n",
      "\n",
      "\n",
      "toxic_sent: they're tragic, not ridiculous.\n",
      "neutral_sent: tragic, never comic.\n",
      "prediction: they're not funny.\n",
      "\n",
      "\n",
      "toxic_sent: so nelson was crazy, like you said.\n",
      "neutral_sent: so nelson was a control freak like you said.\n",
      "prediction: so crazy was crazy, you crazy.\n",
      "\n",
      "\n",
      "toxic_sent: i mean, this is retarded.\n",
      "neutral_sent: i mean, this is crazy.\n",
      "prediction: i mean, this is mean.\n",
      "\n",
      "\n",
      "toxic_sent: it's useless!\n",
      "neutral_sent: this is futile!\n",
      "prediction: it's no use!\n",
      "\n",
      "\n",
      "toxic_sent: cunning fox, this ernie allen.\n",
      "neutral_sent: a clever old fox, ernie allen.\n",
      "prediction: fox fox, fox fox.\n",
      "\n",
      "\n",
      "toxic_sent: fuck! let me out of here!\n",
      "neutral_sent: get me out of here!\n",
      "prediction: let me out of here!\n",
      "\n",
      "\n",
      "toxic_sent: i'm going to arrest you.\n",
      "neutral_sent: here to arrest you.\n",
      "prediction: i'll arrest you.\n",
      "\n",
      "\n",
      "toxic_sent: grotesque as i promised.\n",
      "neutral_sent: grotesque, as promised . - okay.\n",
      "prediction: i promised promised promised.\n",
      "\n",
      "\n",
      "toxic_sent: oh, shit! it's a collapsible circuit.\n",
      "neutral_sent: it's a hell of a circuit.\n",
      "prediction: oh, it's a.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "detokenizer = TreebankWordDetokenizer()\n",
    "\n",
    "# let's see how our model works\n",
    "num_examples = 10\n",
    "for _ in range(num_examples):\n",
    "    idx = val_idx[np.random.randint(0, len(val_idx))]\n",
    "    toxic_sent = detokenizer.detokenize(df.loc[val_idx, 'toxic_sent'][idx])\n",
    "    neutral_sent = detokenizer.detokenize(df.loc[val_idx, 'neutral_sent'][idx])\n",
    "    \n",
    "    print('toxic_sent:', toxic_sent)\n",
    "    print('neutral_sent:', neutral_sent)\n",
    "    print('prediction:', model.predict(toxic_sent))\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
