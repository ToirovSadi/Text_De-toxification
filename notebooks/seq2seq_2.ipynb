{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f55b4fd-f102-4240-909e-930bac4c4683",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_MODEL = 'models/seq2seq_2.01.pt'\n",
    "MODEL_CHECKPOINT = 'models/seq2seq_2.01.pt'\n",
    "DATASET_PATH = 'data/interim/preprocessed_paranmt3.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40068414-5b9f-4586-bcdb-470c3efb7c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os\n",
    "os.chdir(\"..\") # go to the root dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf16aec9-3a08-4294-a90e-0fba120ccfde",
   "metadata": {},
   "source": [
    "# Get the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e44e86ca-4ca7-4594-9ce7-3dc4c457cf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENT_SIZE = 10\n",
    "MAX_TOKENS = 30_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7abeadd3-8c26-4b70-987b-be5e16ebdf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.make_dataset import ParanmtDataset\n",
    "\n",
    "train_dataset = ParanmtDataset(\n",
    "    path=DATASET_PATH,\n",
    "    max_sent_size=MAX_SENT_SIZE,\n",
    "    train=True,\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9ae95c9-af8b-4d30-a1e6-633cf41d5641",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.build_vocab(\n",
    "    min_freq=2,\n",
    "    specials=['<unk>', '<pad>', '<sos>', '<eos>'],\n",
    "    max_tokens=MAX_TOKENS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f9475a6-7517-48ac-a33d-b3db03c0dafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of encoder vocab: 12137\n",
      "size of decoder vocab: 14544\n"
     ]
    }
   ],
   "source": [
    "enc_vocab = train_dataset.toxic_vocab\n",
    "dec_vocab = train_dataset.neutral_vocab\n",
    "\n",
    "print(\"size of encoder vocab:\", len(enc_vocab))\n",
    "print(\"size of decoder vocab:\", len(dec_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c1b8e3e-a586-4ed7-96c9-b9b98e983a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = ParanmtDataset(\n",
    "    path=DATASET_PATH,\n",
    "    max_sent_size=MAX_SENT_SIZE,\n",
    "    vocabs=(enc_vocab, dec_vocab), # avoid data leakage\n",
    "    train=False,\n",
    "    seed=42,\n",
    "    take_first=10_000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88e98520-9f4a-439e-ab9d-6db44cfe64bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>similarity</th>\n",
       "      <th>lenght_diff</th>\n",
       "      <th>toxic_sent</th>\n",
       "      <th>neutral_sent</th>\n",
       "      <th>toxic_val</th>\n",
       "      <th>neutral_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.784351</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>[dressing, like, a, bum, ?]</td>\n",
       "      <td>[walk, like, a, bum, ?]</td>\n",
       "      <td>0.911522</td>\n",
       "      <td>0.040193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.610479</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[you, bum, !]</td>\n",
       "      <td>[slacker, !]</td>\n",
       "      <td>0.968336</td>\n",
       "      <td>0.006487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.866624</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>[chloe, ,, stop, being, so, paranoid, .]</td>\n",
       "      <td>[chloe, ,, stop, being, so, paranoid, .]</td>\n",
       "      <td>0.611291</td>\n",
       "      <td>0.021283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.949912</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>[it, stinks, in, here, .]</td>\n",
       "      <td>[stinks, in, here, .]</td>\n",
       "      <td>0.698503</td>\n",
       "      <td>0.013900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.885716</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>[i, want, to, silence, you, .]</td>\n",
       "      <td>[i, silence, you, .]</td>\n",
       "      <td>0.921562</td>\n",
       "      <td>0.020940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162956</th>\n",
       "      <td>0.816062</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>[that, is, a, stupid, excuse, !]</td>\n",
       "      <td>[that, is, a, cowardly, excuse, !]</td>\n",
       "      <td>0.999643</td>\n",
       "      <td>0.042241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162957</th>\n",
       "      <td>0.756084</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>[what, the, hell, ?, huh, ?]</td>\n",
       "      <td>[what, is, wrong, ?]</td>\n",
       "      <td>0.825482</td>\n",
       "      <td>0.000042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162958</th>\n",
       "      <td>0.618064</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>[like, your, pecker, .]</td>\n",
       "      <td>[like, a, cue, .]</td>\n",
       "      <td>0.978997</td>\n",
       "      <td>0.000061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162959</th>\n",
       "      <td>0.687292</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>[my, friend, i, asexual, beast, .]</td>\n",
       "      <td>[my, girl, i, asexual, bestie, .]</td>\n",
       "      <td>0.997259</td>\n",
       "      <td>0.071518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162960</th>\n",
       "      <td>0.630208</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>[i, mean, ,, my, god, -, fuck, .]</td>\n",
       "      <td>[i, mean, ,, for, heaven, i, sake]</td>\n",
       "      <td>0.999544</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>162961 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        similarity  lenght_diff                                toxic_sent  \\\n",
       "0         0.784351     0.190476               [dressing, like, a, bum, ?]   \n",
       "1         0.610479     0.000000                             [you, bum, !]   \n",
       "2         0.866624     0.060606  [chloe, ,, stop, being, so, paranoid, .]   \n",
       "3         0.949912     0.157895                 [it, stinks, in, here, .]   \n",
       "4         0.885716     0.347826            [i, want, to, silence, you, .]   \n",
       "...            ...          ...                                       ...   \n",
       "162956    0.816062     0.076923          [that, is, a, stupid, excuse, !]   \n",
       "162957    0.756084     0.300000              [what, the, hell, ?, huh, ?]   \n",
       "162958    0.618064     0.333333                   [like, your, pecker, .]   \n",
       "162959    0.687292     0.037037        [my, friend, i, asexual, beast, .]   \n",
       "162960    0.630208     0.103448         [i, mean, ,, my, god, -, fuck, .]   \n",
       "\n",
       "                                    neutral_sent  toxic_val  neutral_val  \n",
       "0                        [walk, like, a, bum, ?]   0.911522     0.040193  \n",
       "1                                   [slacker, !]   0.968336     0.006487  \n",
       "2       [chloe, ,, stop, being, so, paranoid, .]   0.611291     0.021283  \n",
       "3                          [stinks, in, here, .]   0.698503     0.013900  \n",
       "4                           [i, silence, you, .]   0.921562     0.020940  \n",
       "...                                          ...        ...          ...  \n",
       "162956        [that, is, a, cowardly, excuse, !]   0.999643     0.042241  \n",
       "162957                      [what, is, wrong, ?]   0.825482     0.000042  \n",
       "162958                         [like, a, cue, .]   0.978997     0.000061  \n",
       "162959         [my, girl, i, asexual, bestie, .]   0.997259     0.071518  \n",
       "162960        [i, mean, ,, for, heaven, i, sake]   0.999544     0.000100  \n",
       "\n",
       "[162961 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6952c83-54f7-40e6-a037-271dff3c5f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(162961, 10000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd8b060-2108-4d18-865c-f9de2ae7969c",
   "metadata": {},
   "source": [
    "## Build the Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f82c34f-8f03-45ae-8eae-02dd2759bf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cad94d9c-7a1f-4ddb-b4a9-1647ab9951bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7953a61f-64c0-49a3-9fda-f64e80861fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic_sent.shape: torch.Size([128, 10])\n",
      "neutral_sent.shape: torch.Size([128, 10])\n"
     ]
    }
   ],
   "source": [
    "# let's check if shape and everything is ok\n",
    "for batch in train_dataloader:\n",
    "    toxic_sent, neutral_sent = batch\n",
    "    print(\"toxic_sent.shape:\", toxic_sent.shape)\n",
    "    print(\"neutral_sent.shape:\", neutral_sent.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dca808e2-529f-4c8e-b035-3b03ec7d053d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc46dfbe-d834-48e7-ad09-c49c2a676386",
   "metadata": {},
   "source": [
    "# Load the Model\n",
    "\n",
    "- Simple EncoderDecoder (Seq2Seq) architerture\n",
    "- a little trick was used (every time the decoder carries with it context vector from the encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08d88aaa-7e97-411f-9164-4bdbfae09a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.seq2seq.encoder import Encoder\n",
    "from src.models.seq2seq.decoder2 import Decoder2 # NOTE: using different Decoder than first notebook\n",
    "from src.models.seq2seq import Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "451a159a-dae5-487c-be50-c87b7407add1",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(enc_vocab)\n",
    "OUTPUT_DIM = len(dec_vocab)\n",
    "EMBED_DIM = 128\n",
    "NUM_HIDDEN = 256\n",
    "N_LAYERS = 1\n",
    "DROPOUT = 0.3\n",
    "ENC_PADDING_IDX = enc_vocab['<pad>']\n",
    "DEC_PADDING_IDX = dec_vocab['<pad>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b48dfdf-ba48-477c-ac62-2c512a69d86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the encoder and decoder for our model\n",
    "encoder = Encoder(\n",
    "    input_dim=INPUT_DIM,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    hidden_dim=NUM_HIDDEN,\n",
    "    num_layers=N_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    vocab=enc_vocab,\n",
    "    padding_idx=ENC_PADDING_IDX\n",
    ").to(device)\n",
    "\n",
    "decoder = Decoder2(\n",
    "    output_dim=OUTPUT_DIM,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    hidden_dim=NUM_HIDDEN,\n",
    "    num_layers=N_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    vocab=dec_vocab,\n",
    "    padding_idx=DEC_PADDING_IDX\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb68fa7c-f8ee-4667-9b20-a638de4fe311",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = float('inf')\n",
    "\n",
    "model = Seq2Seq(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    device=device,\n",
    "    max_sent_size=MAX_SENT_SIZE,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "edbb2267-04cd-4690-97ad-072c94b2bd3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters in model: 19.0M\n"
     ]
    }
   ],
   "source": [
    "from src.models.utils import count_parameters\n",
    "\n",
    "print(f\"number of parameters in model: {count_parameters(model)//1e6}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f8fc5fb-f477-4574-9c97-6d33ece94f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load(LOAD_MODEL)\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "088f446e-d95e-4812-949d-a3a5eb20ab6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=dec_vocab['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae355174-b99f-403c-b926-def9bd694c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1274/1274 [00:40<00:00, 31.68it/s, loss=3.48]\n",
      "Evaluating 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:00<00:00, 94.12it/s, loss=4.92]\n",
      "Training 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1274/1274 [00:40<00:00, 31.22it/s, loss=2.81]\n",
      "Evaluating 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:00<00:00, 91.25it/s, loss=4.72]\n",
      "Training 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1274/1274 [00:40<00:00, 31.22it/s, loss=2.55]\n",
      "Evaluating 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:00<00:00, 81.78it/s, loss=4.86]\n",
      "Training 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1274/1274 [00:40<00:00, 31.12it/s, loss=2.36]\n",
      "Evaluating 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:00<00:00, 95.30it/s, loss=5.07]\n",
      "Training 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1274/1274 [00:40<00:00, 31.25it/s, loss=2.21]\n",
      "Evaluating 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:00<00:00, 93.65it/s, loss=5.11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update teacher force to 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1274/1274 [00:40<00:00, 31.36it/s, loss=3.31]\n",
      "Evaluating 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:00<00:00, 94.50it/s, loss=3.37]\n",
      "Training 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1274/1274 [00:40<00:00, 31.40it/s, loss=3.1]\n",
      "Evaluating 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:00<00:00, 92.83it/s, loss=3.34]\n",
      "Training 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1274/1274 [00:40<00:00, 31.22it/s, loss=2.97]\n",
      "Evaluating 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:00<00:00, 99.72it/s, loss=3.36] \n",
      "Training 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1274/1274 [00:40<00:00, 31.24it/s, loss=2.87]\n",
      "Evaluating 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:00<00:00, 97.11it/s, loss=3.36] \n",
      "Training 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1274/1274 [00:40<00:00, 31.38it/s, loss=2.78]\n",
      "Evaluating 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:00<00:00, 95.35it/s, loss=3.41]\n",
      "Training 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1274/1274 [00:40<00:00, 31.38it/s, loss=2.7]\n",
      "Evaluating 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:00<00:00, 94.83it/s, loss=3.46]\n",
      "Training 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1274/1274 [00:40<00:00, 31.48it/s, loss=2.63]\n",
      "Evaluating 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:00<00:00, 89.06it/s, loss=3.47]\n",
      "Training 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1274/1274 [00:41<00:00, 31.03it/s, loss=2.57]\n",
      "Evaluating 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:00<00:00, 98.12it/s, loss=3.52]\n",
      "Training 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1274/1274 [00:40<00:00, 31.27it/s, loss=2.51]\n",
      "Evaluating 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:00<00:00, 91.91it/s, loss=3.57]\n",
      "Training 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1274/1274 [00:40<00:00, 31.19it/s, loss=2.46]\n",
      "Evaluating 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:00<00:00, 96.25it/s, loss=3.56]\n",
      "Training 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1274/1274 [00:41<00:00, 30.99it/s, loss=2.41]\n",
      "Evaluating 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:00<00:00, 90.43it/s, loss=3.64]\n",
      "Training 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1274/1274 [00:40<00:00, 31.33it/s, loss=2.37]\n",
      "Evaluating 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:00<00:00, 97.59it/s, loss=3.66] \n",
      "Training 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1274/1274 [00:40<00:00, 31.40it/s, loss=2.33]\n",
      "Evaluating 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:00<00:00, 92.37it/s, loss=3.7] \n",
      "Training 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1274/1274 [00:40<00:00, 31.24it/s, loss=2.3] \n",
      "Evaluating 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:00<00:00, 100.05it/s, loss=3.73]\n",
      "Training 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1274/1274 [00:40<00:00, 31.27it/s, loss=2.27]\n",
      "Evaluating 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:00<00:00, 89.59it/s, loss=3.72]\n"
     ]
    }
   ],
   "source": [
    "from src.models.train_model import train\n",
    "\n",
    "best_loss = train(\n",
    "    model=model,\n",
    "    loaders=(train_dataloader, val_dataloader),\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    epochs=20,\n",
    "    device=device,\n",
    "    best_loss=best_loss,\n",
    "    ckpt_path=MODEL_CHECKPOINT,\n",
    "    clip_grad=1,\n",
    "    teacher_force={\n",
    "        'value': 1,\n",
    "        'gamma': 0,\n",
    "        'update_every_n_epoch': 5,\n",
    "    } # first 5 epoch teacher force 1, after it will be turned off\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7dc1b6ad-901d-4582-84c8-79ba34fdcebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # let's load the model and predict\n",
    "# model = torch.load(MODEL_CHECKPOINT)\n",
    "# model.to(device)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e3f96417-66e1-4061-ba49-55207e953142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic_sent: if you die\n",
      "neutral_sent: even if you died\n",
      "predictions:\n",
      "\t1) ['if', 'you', 'die', '<eos>']\n",
      "\t2) ['if', 'you', 'die', 'die', '<eos>']\n",
      "\t3) ['when', 'you', 'die', '<eos>']\n",
      "\n",
      "\n",
      "toxic_sent: i am going to die\n",
      "neutral_sent: will i die here will i\n",
      "predictions:\n",
      "\t1) ['i', 'am', 'die', '<eos>']\n",
      "\t2) ['i', 'am', 'dying', '<eos>']\n",
      "\t3) ['i', 'am', 'die', 'to', '<eos>']\n",
      "\n",
      "\n",
      "toxic_sent: paula i crazy come on.\n",
      "neutral_sent: paulie, this is crazy.\n",
      "predictions:\n",
      "\t1) ['paulie', 'i', 'crazy', 'crazy', '.', '.', '<eos>']\n",
      "\t2) ['paulie', 'i', 'crazy', 'on', '.', '.', '<eos>']\n",
      "\t3) ['paulie', 'i', 'crazy', 'crazy', '.', '<eos>']\n",
      "\n",
      "\n",
      "toxic_sent: drunk my ass.\n",
      "neutral_sent: you are drunk.\n",
      "predictions:\n",
      "\t1) ['drunk', 'my', '.', '.', '<eos>']\n",
      "\t2) ['drunk', 'my', '.', '.', '.', '<eos>']\n",
      "\t3) ['drunk', '.', '.', '.', '.', '<eos>']\n",
      "\n",
      "\n",
      "toxic_sent: are you crazy?\n",
      "neutral_sent: merlin! are you mad?\n",
      "predictions:\n",
      "\t1) ['are', 'you', 'mad', '?', '<eos>']\n",
      "\t2) ['are', 'you', 'mad', '?', '?', '<eos>']\n",
      "\t3) ['have', 'you', 'mad', '?', '<eos>']\n",
      "\n",
      "\n",
      "toxic_sent: bring every fucking gun we have.\n",
      "neutral_sent: take all the weapons we have!\n",
      "predictions:\n",
      "\t1) ['take', 'the', 'weapons', 'weapons', 'guns', '.', '.', '.', '<eos>']\n",
      "\t2) ['bring', 'the', 'weapons', 'weapons', 'guns', '.', '.', '.', '<eos>']\n",
      "\t3) ['get', 'the', 'weapons', 'weapons', 'guns', '.', '.', '.', '<eos>']\n",
      "\n",
      "\n",
      "toxic_sent: every witch is born into a circle.\n",
      "neutral_sent: every wizard is born into the circle.\n",
      "predictions:\n",
      "\t1) ['every', 'wizard', 'is', 'born', 'a', '.', 'circle', '.', '<eos>']\n",
      "\t2) ['every', 'wizard', 'is', 'born', 'a', '.', '.', '.', '<eos>']\n",
      "\t3) ['every', 'wizard', 'is', 'born', 'a', 'the', 'circle', '.', '<eos>']\n",
      "\n",
      "\n",
      "toxic_sent: i am going to die.\n",
      "neutral_sent: i am dying!\n",
      "predictions:\n",
      "\t1) ['i', 'am', 'dying', '<eos>']\n",
      "\t2) ['i', 'am', 'dying', '.', '<eos>']\n",
      "\t3) ['i', 'am', 'dying', 'to', '<eos>']\n",
      "\n",
      "\n",
      "toxic_sent: damn it, andy.\n",
      "neutral_sent: hell, andy.\n",
      "predictions:\n",
      "\t1) ['hell', ',', 'andy', '.', '<eos>']\n",
      "\t2) ['oh', ',', 'andy', '.', '<eos>']\n",
      "\t3) ['hell', ',', 'andy', ',', '<eos>']\n",
      "\n",
      "\n",
      "toxic_sent: hug your mom.\n",
      "neutral_sent: give your mama a hug.\n",
      "predictions:\n",
      "\t1) ['give', 'hug', 'your', 'hug', 'hug', '<eos>']\n",
      "\t2) ['give', 'your', 'hug', 'hug', 'hug', '<eos>']\n",
      "\t3) ['give', 'hug', 'your', '.', 'hug', '<eos>']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "detokenizer = TreebankWordDetokenizer()\n",
    "\n",
    "# let's see how our model works\n",
    "num_examples = 10\n",
    "num_sentence = 3\n",
    "dataset = train_dataset\n",
    "for _ in range(num_examples):\n",
    "    idx = np.random.randint(0, len(dataset))\n",
    "    toxic_sent = detokenizer.detokenize(dataset.df.loc[idx, 'toxic_sent'])\n",
    "    neutral_sent = detokenizer.detokenize(dataset.df.loc[idx, 'neutral_sent'])\n",
    "    \n",
    "    print('toxic_sent:', toxic_sent)\n",
    "    print('neutral_sent:', neutral_sent)\n",
    "    # let's use beam search\n",
    "    preds = model.predict(\n",
    "        toxic_sent,\n",
    "        beam=True,\n",
    "        beam_search_num_candidates=num_sentence,\n",
    "        post_process_text=False,\n",
    "    )\n",
    "    print(\"predictions:\")\n",
    "    for i in range(num_sentence):\n",
    "        print(f\"\\t{i+1})\", preds[i])\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
