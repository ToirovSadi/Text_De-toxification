{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40068414-5b9f-4586-bcdb-470c3efb7c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "os.chdir(\"..\") # go to the root dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e44e86ca-4ca7-4594-9ce7-3dc4c457cf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('data/interim/preprocessed_paranmt.tsv', sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7abeadd3-8c26-4b70-987b-be5e16ebdf6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>similarity</th>\n",
       "      <th>lenght_diff</th>\n",
       "      <th>toxic_sent</th>\n",
       "      <th>neutral_sent</th>\n",
       "      <th>toxic_val</th>\n",
       "      <th>neutral_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.785171</td>\n",
       "      <td>0.010309</td>\n",
       "      <td>if alkar floods her with her mental waste , it...</td>\n",
       "      <td>if alkar is flooding her with psychic waste , ...</td>\n",
       "      <td>0.981983</td>\n",
       "      <td>0.014195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.749687</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>you 're becoming disgusting .</td>\n",
       "      <td>now you 're getting nasty .</td>\n",
       "      <td>0.999039</td>\n",
       "      <td>0.065473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.919051</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>well , we can spare your life .</td>\n",
       "      <td>well , we could spare your life , for one .</td>\n",
       "      <td>0.985068</td>\n",
       "      <td>0.213313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.664333</td>\n",
       "      <td>0.309524</td>\n",
       "      <td>monkey , you have to wake up .</td>\n",
       "      <td>ah ! monkey , you 've got to snap out of it .</td>\n",
       "      <td>0.994215</td>\n",
       "      <td>0.053362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.726639</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>i have orders to kill her .</td>\n",
       "      <td>i 've got orders to put her down .</td>\n",
       "      <td>0.999348</td>\n",
       "      <td>0.009402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577772</th>\n",
       "      <td>0.870322</td>\n",
       "      <td>0.030769</td>\n",
       "      <td>you did n't know that estelle stole your fish ...</td>\n",
       "      <td>you did n't know that estelle had stolen some ...</td>\n",
       "      <td>0.949143</td>\n",
       "      <td>0.000121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577773</th>\n",
       "      <td>0.722897</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>it'il suck the life out of you !</td>\n",
       "      <td>you 'd be sucked out of your life !</td>\n",
       "      <td>0.996124</td>\n",
       "      <td>0.215794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577774</th>\n",
       "      <td>0.617511</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>i ca n't fuckin ' take that , bruv .</td>\n",
       "      <td>i really ca n't take this .</td>\n",
       "      <td>0.984538</td>\n",
       "      <td>0.000049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577775</th>\n",
       "      <td>0.679613</td>\n",
       "      <td>0.358209</td>\n",
       "      <td>they called me a fucking hero . the truth is i...</td>\n",
       "      <td>they said i was a hero , but i did n't care .</td>\n",
       "      <td>0.991945</td>\n",
       "      <td>0.000124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577776</th>\n",
       "      <td>0.868475</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>i did n't fuck him .</td>\n",
       "      <td>i did not screw him .</td>\n",
       "      <td>0.994174</td>\n",
       "      <td>0.009480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>577777 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        similarity  lenght_diff  \\\n",
       "0         0.785171     0.010309   \n",
       "1         0.749687     0.071429   \n",
       "2         0.919051     0.268293   \n",
       "3         0.664333     0.309524   \n",
       "4         0.726639     0.181818   \n",
       "...            ...          ...   \n",
       "577772    0.870322     0.030769   \n",
       "577773    0.722897     0.058824   \n",
       "577774    0.617511     0.212121   \n",
       "577775    0.679613     0.358209   \n",
       "577776    0.868475     0.095238   \n",
       "\n",
       "                                               toxic_sent  \\\n",
       "0       if alkar floods her with her mental waste , it...   \n",
       "1                           you 're becoming disgusting .   \n",
       "2                         well , we can spare your life .   \n",
       "3                          monkey , you have to wake up .   \n",
       "4                             i have orders to kill her .   \n",
       "...                                                   ...   \n",
       "577772  you did n't know that estelle stole your fish ...   \n",
       "577773                   it'il suck the life out of you !   \n",
       "577774               i ca n't fuckin ' take that , bruv .   \n",
       "577775  they called me a fucking hero . the truth is i...   \n",
       "577776                               i did n't fuck him .   \n",
       "\n",
       "                                             neutral_sent  toxic_val  \\\n",
       "0       if alkar is flooding her with psychic waste , ...   0.981983   \n",
       "1                             now you 're getting nasty .   0.999039   \n",
       "2             well , we could spare your life , for one .   0.985068   \n",
       "3           ah ! monkey , you 've got to snap out of it .   0.994215   \n",
       "4                      i 've got orders to put her down .   0.999348   \n",
       "...                                                   ...        ...   \n",
       "577772  you did n't know that estelle had stolen some ...   0.949143   \n",
       "577773                you 'd be sucked out of your life !   0.996124   \n",
       "577774                        i really ca n't take this .   0.984538   \n",
       "577775      they said i was a hero , but i did n't care .   0.991945   \n",
       "577776                              i did not screw him .   0.994174   \n",
       "\n",
       "        neutral_val  \n",
       "0          0.014195  \n",
       "1          0.065473  \n",
       "2          0.213313  \n",
       "3          0.053362  \n",
       "4          0.009402  \n",
       "...             ...  \n",
       "577772     0.000121  \n",
       "577773     0.215794  \n",
       "577774     0.000049  \n",
       "577775     0.000124  \n",
       "577776     0.009480  \n",
       "\n",
       "[577777 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7007694b-0541-431d-8ac5-0170e92bec9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's convert columns toxic_sent and neutral_sent as a list\n",
    "df = dataframe.copy()\n",
    "df.loc[:, 'toxic_sent'] = df['toxic_sent'].apply(lambda x: x.split(' '))\n",
    "df.loc[:, 'neutral_sent'] = df['neutral_sent'].apply(lambda x: x.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "017bc099-f6a5-4a2a-8e8c-0e73dd2a6ef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>similarity</th>\n",
       "      <th>lenght_diff</th>\n",
       "      <th>toxic_sent</th>\n",
       "      <th>neutral_sent</th>\n",
       "      <th>toxic_val</th>\n",
       "      <th>neutral_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.785171</td>\n",
       "      <td>0.010309</td>\n",
       "      <td>[if, alkar, floods, her, with, her, mental, wa...</td>\n",
       "      <td>[if, alkar, is, flooding, her, with, psychic, ...</td>\n",
       "      <td>0.981983</td>\n",
       "      <td>0.014195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.749687</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>[you, 're, becoming, disgusting, .]</td>\n",
       "      <td>[now, you, 're, getting, nasty, .]</td>\n",
       "      <td>0.999039</td>\n",
       "      <td>0.065473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.919051</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>[well, ,, we, can, spare, your, life, .]</td>\n",
       "      <td>[well, ,, we, could, spare, your, life, ,, for...</td>\n",
       "      <td>0.985068</td>\n",
       "      <td>0.213313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.664333</td>\n",
       "      <td>0.309524</td>\n",
       "      <td>[monkey, ,, you, have, to, wake, up, .]</td>\n",
       "      <td>[ah, !, monkey, ,, you, 've, got, to, snap, ou...</td>\n",
       "      <td>0.994215</td>\n",
       "      <td>0.053362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.726639</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>[i, have, orders, to, kill, her, .]</td>\n",
       "      <td>[i, 've, got, orders, to, put, her, down, .]</td>\n",
       "      <td>0.999348</td>\n",
       "      <td>0.009402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   similarity  lenght_diff                                         toxic_sent  \\\n",
       "0    0.785171     0.010309  [if, alkar, floods, her, with, her, mental, wa...   \n",
       "1    0.749687     0.071429                [you, 're, becoming, disgusting, .]   \n",
       "2    0.919051     0.268293           [well, ,, we, can, spare, your, life, .]   \n",
       "3    0.664333     0.309524            [monkey, ,, you, have, to, wake, up, .]   \n",
       "4    0.726639     0.181818                [i, have, orders, to, kill, her, .]   \n",
       "\n",
       "                                        neutral_sent  toxic_val  neutral_val  \n",
       "0  [if, alkar, is, flooding, her, with, psychic, ...   0.981983     0.014195  \n",
       "1                 [now, you, 're, getting, nasty, .]   0.999039     0.065473  \n",
       "2  [well, ,, we, could, spare, your, life, ,, for...   0.985068     0.213313  \n",
       "3  [ah, !, monkey, ,, you, 've, got, to, snap, ou...   0.994215     0.053362  \n",
       "4       [i, 've, got, orders, to, put, her, down, .]   0.999348     0.009402  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ad89789-130c-4c61-a331-a0bb97315644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the column with sentence size more than MAX_SENT_SIZE (for speed)\n",
    "\n",
    "MAX_SENT_SIZE = 32\n",
    "\n",
    "df = df[df['toxic_sent'].apply(len) <= (MAX_SENT_SIZE-2)]\n",
    "df = df[df['neutral_sent'].apply(len) <= (MAX_SENT_SIZE-2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "301a534b-d4c3-4893-9c32-cccaceda8e22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>similarity</th>\n",
       "      <th>lenght_diff</th>\n",
       "      <th>toxic_sent</th>\n",
       "      <th>neutral_sent</th>\n",
       "      <th>toxic_val</th>\n",
       "      <th>neutral_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.785171</td>\n",
       "      <td>0.010309</td>\n",
       "      <td>[if, alkar, floods, her, with, her, mental, wa...</td>\n",
       "      <td>[if, alkar, is, flooding, her, with, psychic, ...</td>\n",
       "      <td>0.981983</td>\n",
       "      <td>0.014195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.749687</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>[you, 're, becoming, disgusting, .]</td>\n",
       "      <td>[now, you, 're, getting, nasty, .]</td>\n",
       "      <td>0.999039</td>\n",
       "      <td>0.065473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.919051</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>[well, ,, we, can, spare, your, life, .]</td>\n",
       "      <td>[well, ,, we, could, spare, your, life, ,, for...</td>\n",
       "      <td>0.985068</td>\n",
       "      <td>0.213313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.664333</td>\n",
       "      <td>0.309524</td>\n",
       "      <td>[monkey, ,, you, have, to, wake, up, .]</td>\n",
       "      <td>[ah, !, monkey, ,, you, 've, got, to, snap, ou...</td>\n",
       "      <td>0.994215</td>\n",
       "      <td>0.053362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.726639</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>[i, have, orders, to, kill, her, .]</td>\n",
       "      <td>[i, 've, got, orders, to, put, her, down, .]</td>\n",
       "      <td>0.999348</td>\n",
       "      <td>0.009402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577772</th>\n",
       "      <td>0.870322</td>\n",
       "      <td>0.030769</td>\n",
       "      <td>[you, did, n't, know, that, estelle, stole, yo...</td>\n",
       "      <td>[you, did, n't, know, that, estelle, had, stol...</td>\n",
       "      <td>0.949143</td>\n",
       "      <td>0.000121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577773</th>\n",
       "      <td>0.722897</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>[it'il, suck, the, life, out, of, you, !]</td>\n",
       "      <td>[you, 'd, be, sucked, out, of, your, life, !]</td>\n",
       "      <td>0.996124</td>\n",
       "      <td>0.215794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577774</th>\n",
       "      <td>0.617511</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>[i, ca, n't, fuckin, ', take, that, ,, bruv, .]</td>\n",
       "      <td>[i, really, ca, n't, take, this, .]</td>\n",
       "      <td>0.984538</td>\n",
       "      <td>0.000049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577775</th>\n",
       "      <td>0.679613</td>\n",
       "      <td>0.358209</td>\n",
       "      <td>[they, called, me, a, fucking, hero, ., the, t...</td>\n",
       "      <td>[they, said, i, was, a, hero, ,, but, i, did, ...</td>\n",
       "      <td>0.991945</td>\n",
       "      <td>0.000124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577776</th>\n",
       "      <td>0.868475</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>[i, did, n't, fuck, him, .]</td>\n",
       "      <td>[i, did, not, screw, him, .]</td>\n",
       "      <td>0.994174</td>\n",
       "      <td>0.009480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>553164 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        similarity  lenght_diff  \\\n",
       "0         0.785171     0.010309   \n",
       "1         0.749687     0.071429   \n",
       "2         0.919051     0.268293   \n",
       "3         0.664333     0.309524   \n",
       "4         0.726639     0.181818   \n",
       "...            ...          ...   \n",
       "577772    0.870322     0.030769   \n",
       "577773    0.722897     0.058824   \n",
       "577774    0.617511     0.212121   \n",
       "577775    0.679613     0.358209   \n",
       "577776    0.868475     0.095238   \n",
       "\n",
       "                                               toxic_sent  \\\n",
       "0       [if, alkar, floods, her, with, her, mental, wa...   \n",
       "1                     [you, 're, becoming, disgusting, .]   \n",
       "2                [well, ,, we, can, spare, your, life, .]   \n",
       "3                 [monkey, ,, you, have, to, wake, up, .]   \n",
       "4                     [i, have, orders, to, kill, her, .]   \n",
       "...                                                   ...   \n",
       "577772  [you, did, n't, know, that, estelle, stole, yo...   \n",
       "577773          [it'il, suck, the, life, out, of, you, !]   \n",
       "577774    [i, ca, n't, fuckin, ', take, that, ,, bruv, .]   \n",
       "577775  [they, called, me, a, fucking, hero, ., the, t...   \n",
       "577776                        [i, did, n't, fuck, him, .]   \n",
       "\n",
       "                                             neutral_sent  toxic_val  \\\n",
       "0       [if, alkar, is, flooding, her, with, psychic, ...   0.981983   \n",
       "1                      [now, you, 're, getting, nasty, .]   0.999039   \n",
       "2       [well, ,, we, could, spare, your, life, ,, for...   0.985068   \n",
       "3       [ah, !, monkey, ,, you, 've, got, to, snap, ou...   0.994215   \n",
       "4            [i, 've, got, orders, to, put, her, down, .]   0.999348   \n",
       "...                                                   ...        ...   \n",
       "577772  [you, did, n't, know, that, estelle, had, stol...   0.949143   \n",
       "577773      [you, 'd, be, sucked, out, of, your, life, !]   0.996124   \n",
       "577774                [i, really, ca, n't, take, this, .]   0.984538   \n",
       "577775  [they, said, i, was, a, hero, ,, but, i, did, ...   0.991945   \n",
       "577776                       [i, did, not, screw, him, .]   0.994174   \n",
       "\n",
       "        neutral_val  \n",
       "0          0.014195  \n",
       "1          0.065473  \n",
       "2          0.213313  \n",
       "3          0.053362  \n",
       "4          0.009402  \n",
       "...             ...  \n",
       "577772     0.000121  \n",
       "577773     0.215794  \n",
       "577774     0.000049  \n",
       "577775     0.000124  \n",
       "577776     0.009480  \n",
       "\n",
       "[553164 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5aa4b3f2-24fe-4728-9217-0077b58bfca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[:150_000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd8b060-2108-4d18-865c-f9de2ae7969c",
   "metadata": {},
   "source": [
    "## Build the Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f82c34f-8f03-45ae-8eae-02dd2759bf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cad94d9c-7a1f-4ddb-b4a9-1647ab9951bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and val\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_idx, val_idx = train_test_split(list(df.index), train_size=0.9, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f34f127-deb1-46da-8198-90182d9ec906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(135000, 15000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_idx), len(val_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa20304-6d4f-457e-bb38-518308c38543",
   "metadata": {},
   "source": [
    "## Let's build the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78dc9b96-f7f9-4d7d-8207-91a0d02b4ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS = 25_000 # TODO: change this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6286294-8b5e-4616-88c6-1bcd4b62e55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "def yield_tokens(df):\n",
    "    for i in df.index:\n",
    "        toxic_sent = df['toxic_sent'][i]\n",
    "        yield toxic_sent\n",
    "        \n",
    "        neutral_sent = df['neutral_sent'][i]\n",
    "        yield list(neutral_sent)\n",
    "\n",
    "\n",
    "UNK_IDX = 0\n",
    "vocab = build_vocab_from_iterator(\n",
    "    yield_tokens(df.loc[train_idx]), # to avoid data leakage\n",
    "    min_freq=2,\n",
    "    specials=['<unk>', '<pad>', '<sos>', '<eos>'],\n",
    "    max_tokens=MAX_TOKENS,\n",
    ")\n",
    "vocab.set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2e82fb8-2085-4127-9c79-7780467059ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dca808e2-529f-4c8e-b035-3b03ec7d053d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7d5e2e4-dce6-470f-9f5d-06a09c798d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    toxic_sent, neutral_sent = [], []\n",
    "    toxic_val, neutral_val = [], []\n",
    "    similarity, len_diff = [], []\n",
    "    for b in batch:\n",
    "        _similarity, _len_diff, _toxic_sent, _neutral_sent, _toxic_val, _neutral_val = b\n",
    "        similarity.append(_similarity)\n",
    "        len_diff.append(_len_diff)\n",
    "        toxic_val.append(_toxic_val)\n",
    "        neutral_val.append(_neutral_val)\n",
    "        \n",
    "        # add <sos> and <eos> to _toxic_sent and _neutral_sent\n",
    "        _toxic_sent = ['<sos>'] + _toxic_sent[:MAX_SENT_SIZE-2] + ['<eos>']\n",
    "        _neutral_sent = ['<sos>'] + _neutral_sent[:MAX_SENT_SIZE-2] + ['<eos>']\n",
    "        \n",
    "        _toxic_sent = vocab.lookup_indices(_toxic_sent)\n",
    "        while len(_toxic_sent) < MAX_SENT_SIZE:\n",
    "            _toxic_sent.append(vocab['<pad>'])\n",
    "        \n",
    "        _neutral_sent = vocab.lookup_indices(_neutral_sent)\n",
    "        while len(_neutral_sent) < MAX_SENT_SIZE:\n",
    "            _neutral_sent.append(vocab['<pad>'])\n",
    "        \n",
    "        toxic_sent.append(torch.tensor(_toxic_sent).reshape(MAX_SENT_SIZE, 1))\n",
    "        neutral_sent.append(torch.tensor(_neutral_sent).reshape(MAX_SENT_SIZE, 1))\n",
    "        \n",
    "    toxic_sent = torch.cat(toxic_sent, dim=1).to(device)\n",
    "    neutral_sent = torch.cat(neutral_sent, dim=1).to(device)\n",
    "    similarity = torch.tensor(similarity, device=device)\n",
    "    len_diff = torch.tensor(len_diff, device=device)\n",
    "    toxic_val = torch.tensor(toxic_val, device=device)\n",
    "    neutral_val = torch.tensor(neutral_val, device=device)\n",
    "    return similarity, len_diff, toxic_sent, neutral_sent, toxic_val, neutral_val\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    df.loc[train_idx].to_numpy(),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batch,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    df.loc[val_idx].to_numpy(),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_batch,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7953a61f-64c0-49a3-9fda-f64e80861fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity.shape: torch.Size([16])\n",
      "len_diff.shape: torch.Size([16])\n",
      "toxic_sent.shape: torch.Size([32, 16])\n",
      "neutral_sent.shape: torch.Size([32, 16])\n",
      "toxic_val.shape: torch.Size([16])\n",
      "neutral_val.shape: torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "# let's check if shape and everything is ok\n",
    "for batch in train_dataloader:\n",
    "    similarity, len_diff, toxic_sent, neutral_sent, toxic_val, neutral_val = batch\n",
    "    print(\"similarity.shape:\", similarity.shape)\n",
    "    print(\"len_diff.shape:\", len_diff.shape)\n",
    "    print(\"toxic_sent.shape:\", toxic_sent.shape)\n",
    "    print(\"neutral_sent.shape:\", neutral_sent.shape)\n",
    "    print(\"toxic_val.shape:\", toxic_val.shape)\n",
    "    print(\"neutral_val.shape:\", neutral_val.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc46dfbe-d834-48e7-ad09-c49c2a676386",
   "metadata": {},
   "source": [
    "# Load the Model\n",
    "\n",
    "- Simple EncoderDecoder (Seq2Seq) architerture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1b9c17a-0069-481e-a76c-240d46bf10a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.attention.encoder import Encoder\n",
    "from src.models.attention.decoder import Decoder\n",
    "from src.models.attention.attention import Attention\n",
    "from src.models.attention import Seq2SeqAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "451a159a-dae5-487c-be50-c87b7407add1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure some parameters for the model\n",
    "padding_idx = vocab['<pad>']\n",
    "\n",
    "## Encoder\n",
    "enc_input_dim = len(vocab)\n",
    "enc_embed_dim = 128\n",
    "enc_hidden_dim = 256\n",
    "enc_dropout = 0.5\n",
    "\n",
    "## Decoder\n",
    "dec_output_dim = len(vocab)\n",
    "dec_embed_dim = 128\n",
    "dec_hidden_dim = 256\n",
    "dec_dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b48dfdf-ba48-477c-ac62-2c512a69d86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the encoder and decoder for our model\n",
    "encoder = Encoder(\n",
    "    input_dim=enc_input_dim,\n",
    "    embed_dim=enc_embed_dim,\n",
    "    hidden_dim=enc_hidden_dim,\n",
    "    dec_hidden_dim=dec_hidden_dim,\n",
    "    dropout=enc_dropout,\n",
    "    padding_idx=padding_idx,\n",
    ").to(device)\n",
    "\n",
    "attention = Attention(\n",
    "    enc_hidden_dim,\n",
    "    dec_hidden_dim,\n",
    ")\n",
    "\n",
    "decoder = Decoder(\n",
    "    output_dim=dec_output_dim,\n",
    "    embed_dim=dec_embed_dim,\n",
    "    hidden_dim=dec_hidden_dim,\n",
    "    attention=attention,\n",
    "    enc_hidden_dim=enc_hidden_dim,\n",
    "    dropout=dec_dropout,\n",
    "    padding_idx=padding_idx,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb68fa7c-f8ee-4667-9b20-a638de4fe311",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = float('inf')\n",
    "\n",
    "model = Seq2SeqAttention(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    device=device,\n",
    "    max_sent_size=MAX_SENT_SIZE,\n",
    "    vocab=vocab,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "088f446e-d95e-4812-949d-a3a5eb20ab6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=3e-4)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae355174-b99f-403c-b926-def9bd694c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8438/8438 [12:26<00:00, 11.31it/s, loss=4.58]\n",
      "Evaluating 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:15<00:00, 60.17it/s, loss=4.43]\n",
      "Training 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8438/8438 [12:26<00:00, 11.31it/s, loss=3.94]\n",
      "Evaluating 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:15<00:00, 59.78it/s, loss=4.32]\n",
      "Training 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8438/8438 [12:25<00:00, 11.31it/s, loss=3.69]\n",
      "Evaluating 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:15<00:00, 60.11it/s, loss=4.31]\n",
      "Training 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8438/8438 [12:26<00:00, 11.30it/s, loss=3.53]\n",
      "Evaluating 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:15<00:00, 59.87it/s, loss=4.32]\n",
      "Training 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8438/8438 [12:27<00:00, 11.29it/s, loss=3.4] \n",
      "Evaluating 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:15<00:00, 59.73it/s, loss=4.31]\n",
      "Training 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8438/8438 [12:26<00:00, 11.30it/s, loss=3.31]\n",
      "Evaluating 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:15<00:00, 59.42it/s, loss=4.29]\n",
      "Training 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8438/8438 [12:26<00:00, 11.30it/s, loss=3.23]\n",
      "Evaluating 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:15<00:00, 59.77it/s, loss=4.3] \n",
      "Training 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8438/8438 [12:26<00:00, 11.31it/s, loss=3.16]\n",
      "Evaluating 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:15<00:00, 59.59it/s, loss=4.3] \n",
      "Training 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8438/8438 [12:26<00:00, 11.30it/s, loss=3.1] \n",
      "Evaluating 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:15<00:00, 60.64it/s, loss=4.34]\n",
      "Training 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8438/8438 [12:27<00:00, 11.29it/s, loss=3.06]\n",
      "Evaluating 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:15<00:00, 59.59it/s, loss=4.29]\n"
     ]
    }
   ],
   "source": [
    "from src.models.train_model import train\n",
    "\n",
    "best_loss = train(\n",
    "    model=model,\n",
    "    loaders=(train_dataloader, val_dataloader),\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    epochs=10,\n",
    "    device=device,\n",
    "    best_loss=best_loss,\n",
    "    ckpt_path='models/attention2.pt',\n",
    "    clip_grad=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7dc1b6ad-901d-4582-84c8-79ba34fdcebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqAttention(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(25000, 128, padding_idx=1)\n",
       "    (rnn): GRU(128, 256, bidirectional=True)\n",
       "    (fc): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(25000, 128, padding_idx=1)\n",
       "    (rnn): GRU(640, 256)\n",
       "    (attention): Attention(\n",
       "      (attn): Linear(in_features=768, out_features=256, bias=True)\n",
       "      (v): Linear(in_features=256, out_features=1, bias=False)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (fc_out): Linear(in_features=896, out_features=25000, bias=True)\n",
       "  )\n",
       "  (vocab): Vocab()\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's load the model and predict\n",
    "model = torch.load('models/attention2.pt')\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e3f96417-66e1-4061-ba49-55207e953142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic_sent: \"i will prove him disgraceful in his own body.\"\n",
      "neutral_sent: \"i will prove his villainy upon his body.\"\n",
      "prediction: \"i'll prove him in his body body.\"\n",
      "\n",
      "\n",
      "toxic_sent: fuck man, i'm so sorry.\n",
      "neutral_sent: man, i'm sorry.\n",
      "prediction: man, i'm sorry.\n",
      "\n",
      "\n",
      "toxic_sent: carl, i want you to jump out of there.\n",
      "neutral_sent: carl, i want you to hop on out of there.\n",
      "prediction: carl, i want you to jump out of there.\n",
      "\n",
      "\n",
      "toxic_sent: tara, let's go . fuck off.\n",
      "neutral_sent: tara, let's go.\n",
      "prediction: tara, let's go.\n",
      "\n",
      "\n",
      "toxic_sent: jantar's too fat, he'd probably push the camera inside.\n",
      "neutral_sent: the amber's too thick . it'll probably push the camera further in.\n",
      "prediction: <unk>'s too thick thick, he probably probably probably probably the the inside.\n",
      "\n",
      "\n",
      "toxic_sent: he was a drug dealer.\n",
      "neutral_sent: he was a drug dealer\n",
      "prediction: he was dealer drug dealer.\n",
      "\n",
      "\n",
      "toxic_sent: damn the french and their comfortable beds!\n",
      "neutral_sent: bloody comfortable french beds!\n",
      "prediction: goddam french french and their!\n",
      "\n",
      "\n",
      "toxic_sent: damn the french and their comfortable beds!\n",
      "neutral_sent: bloody comfortable french beds!\n",
      "prediction: goddam french french and their!\n",
      "\n",
      "\n",
      "toxic_sent: look, i know how to destroy the ghost.\n",
      "neutral_sent: now, look, i know how to destroy the phantom.\n",
      "prediction: look, i know how to destroy the ghost.\n",
      "\n",
      "\n",
      "toxic_sent: killin' a man's not something you can undo.\n",
      "neutral_sent: killing people is not something you can fix.\n",
      "prediction: \"he's not a man, you can do something.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "detokenizer = TreebankWordDetokenizer()\n",
    "\n",
    "# let's see how our model works\n",
    "num_examples = 10\n",
    "for _ in range(num_examples):\n",
    "    idx = val_idx[np.random.randint(0, len(val_idx))]\n",
    "    toxic_sent = detokenizer.detokenize(df.loc[val_idx, 'toxic_sent'][idx])\n",
    "    neutral_sent = detokenizer.detokenize(df.loc[val_idx, 'neutral_sent'][idx])\n",
    "    \n",
    "    print('toxic_sent:', toxic_sent)\n",
    "    print('neutral_sent:', neutral_sent)\n",
    "    print('prediction:', model.predict(toxic_sent))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4554e32-4888-4660-a19e-65409f2e34dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
