{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a6f672e-f6b7-4e94-96a3-e71fc3acb2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_MODEL = 'models/attention2.01.pt'\n",
    "MODEL_CHECKPOINT = 'models/attention2.01.pt'\n",
    "DATASET_PATH = 'data/interim/preprocessed_paranmt.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40068414-5b9f-4586-bcdb-470c3efb7c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os\n",
    "os.chdir(\"..\") # go to the root dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb45299-8b78-4ae2-90be-f962ed587c5c",
   "metadata": {},
   "source": [
    "# Get the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e92fffa6-8fc3-46a0-8d87-587f81e95ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENT_SIZE = 32\n",
    "MAX_TOKENS = 10_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81094a8f-0cb9-436c-9892-d3b28b070dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.make_dataset import ParanmtDataset\n",
    "\n",
    "train_dataset = ParanmtDataset(\n",
    "    path=DATASET_PATH,\n",
    "    max_sent_size=MAX_SENT_SIZE,\n",
    "    train=True,\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f324e5bd-eb23-48aa-adc4-075b17a62ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.build_vocab(\n",
    "    min_freq=2,\n",
    "    specials=['<unk>', '<pad>', '<sos>', '<eos>'],\n",
    "    max_tokens=MAX_TOKENS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aecda01b-5e50-41ff-afcf-7b2a6d8d1080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of encoder vocab: 10000\n",
      "size of decoder vocab: 10000\n"
     ]
    }
   ],
   "source": [
    "enc_vocab = train_dataset.toxic_vocab\n",
    "dec_vocab = train_dataset.neutral_vocab\n",
    "\n",
    "print(\"size of encoder vocab:\", len(enc_vocab))\n",
    "print(\"size of decoder vocab:\", len(dec_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2de9532-e02a-405d-ab46-c0ff7f2049aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = ParanmtDataset(\n",
    "    path=DATASET_PATH,\n",
    "    max_sent_size=MAX_SENT_SIZE,\n",
    "    vocabs=(enc_vocab, dec_vocab), # avoid data leakage\n",
    "    train=False,\n",
    "    seed=42,\n",
    "    take_first=10_000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd018d4e-9907-4e9c-a197-18290ea7cb98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>similarity</th>\n",
       "      <th>lenght_diff</th>\n",
       "      <th>toxic_sent</th>\n",
       "      <th>neutral_sent</th>\n",
       "      <th>toxic_val</th>\n",
       "      <th>neutral_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.811567</td>\n",
       "      <td>0.179487</td>\n",
       "      <td>[you, know, i, hate, that, health, food, shit, .]</td>\n",
       "      <td>[you, know, i, hate, a, healthy, diet, .]</td>\n",
       "      <td>0.999437</td>\n",
       "      <td>0.000569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.883822</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>[what, the, hell, is, going, on, here, ?]</td>\n",
       "      <td>[what, is, going, on, there, ?]</td>\n",
       "      <td>0.877907</td>\n",
       "      <td>0.000041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.769068</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>[she, tried, to, kill, her, own, father, with,...</td>\n",
       "      <td>[however, ,, mike, ,, she, tried, to, beat, hi...</td>\n",
       "      <td>0.966588</td>\n",
       "      <td>0.024886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.823836</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>[have, a, shitty, day, .]</td>\n",
       "      <td>[have, a, bad, day, .]</td>\n",
       "      <td>0.996943</td>\n",
       "      <td>0.000633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.670003</td>\n",
       "      <td>0.320513</td>\n",
       "      <td>[you, ever, think, of, screaming, instead, of,...</td>\n",
       "      <td>[did, it, ever, occur, to, you, to, scream, yo...</td>\n",
       "      <td>0.999311</td>\n",
       "      <td>0.011481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470047</th>\n",
       "      <td>0.945723</td>\n",
       "      <td>0.173077</td>\n",
       "      <td>[I, would, slap, you, even, if, mala, does, no...</td>\n",
       "      <td>[i, would, have, slapped, you, even, if, mala,...</td>\n",
       "      <td>0.987526</td>\n",
       "      <td>0.196128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470048</th>\n",
       "      <td>0.767978</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>[death, to, the, al, fayed, !, (, grunts, )]</td>\n",
       "      <td>[the, death, of, al, fayed, !]</td>\n",
       "      <td>0.997817</td>\n",
       "      <td>0.000219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470049</th>\n",
       "      <td>0.766673</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>[i, think, he, is, manure, ,, wolf, .]</td>\n",
       "      <td>[i, think, he, is, buggered, ,, wolf, .]</td>\n",
       "      <td>0.970698</td>\n",
       "      <td>0.000387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470050</th>\n",
       "      <td>0.776357</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>[can, not, even, take, care, of, your, own, go...</td>\n",
       "      <td>[can, not, you, even, take, care, of, your, so...</td>\n",
       "      <td>0.999640</td>\n",
       "      <td>0.000586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470051</th>\n",
       "      <td>0.867722</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>[if, you, did, not, have, a, pussy, ,, there, ...</td>\n",
       "      <td>[if, you, did, not, have, a, cat, ,, there, wo...</td>\n",
       "      <td>0.999392</td>\n",
       "      <td>0.022108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>470052 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        similarity  lenght_diff  \\\n",
       "0         0.811567     0.179487   \n",
       "1         0.883822     0.250000   \n",
       "2         0.769068     0.303030   \n",
       "3         0.823836     0.157895   \n",
       "4         0.670003     0.320513   \n",
       "...            ...          ...   \n",
       "470047    0.945723     0.173077   \n",
       "470048    0.767978     0.272727   \n",
       "470049    0.766673     0.068966   \n",
       "470050    0.776357     0.173913   \n",
       "470051    0.867722     0.015152   \n",
       "\n",
       "                                               toxic_sent  \\\n",
       "0       [you, know, i, hate, that, health, food, shit, .]   \n",
       "1               [what, the, hell, is, going, on, here, ?]   \n",
       "2       [she, tried, to, kill, her, own, father, with,...   \n",
       "3                               [have, a, shitty, day, .]   \n",
       "4       [you, ever, think, of, screaming, instead, of,...   \n",
       "...                                                   ...   \n",
       "470047  [I, would, slap, you, even, if, mala, does, no...   \n",
       "470048       [death, to, the, al, fayed, !, (, grunts, )]   \n",
       "470049             [i, think, he, is, manure, ,, wolf, .]   \n",
       "470050  [can, not, even, take, care, of, your, own, go...   \n",
       "470051  [if, you, did, not, have, a, pussy, ,, there, ...   \n",
       "\n",
       "                                             neutral_sent  toxic_val  \\\n",
       "0               [you, know, i, hate, a, healthy, diet, .]   0.999437   \n",
       "1                         [what, is, going, on, there, ?]   0.877907   \n",
       "2       [however, ,, mike, ,, she, tried, to, beat, hi...   0.966588   \n",
       "3                                  [have, a, bad, day, .]   0.996943   \n",
       "4       [did, it, ever, occur, to, you, to, scream, yo...   0.999311   \n",
       "...                                                   ...        ...   \n",
       "470047  [i, would, have, slapped, you, even, if, mala,...   0.987526   \n",
       "470048                     [the, death, of, al, fayed, !]   0.997817   \n",
       "470049           [i, think, he, is, buggered, ,, wolf, .]   0.970698   \n",
       "470050  [can, not, you, even, take, care, of, your, so...   0.999640   \n",
       "470051  [if, you, did, not, have, a, cat, ,, there, wo...   0.999392   \n",
       "\n",
       "        neutral_val  \n",
       "0          0.000569  \n",
       "1          0.000041  \n",
       "2          0.024886  \n",
       "3          0.000633  \n",
       "4          0.011481  \n",
       "...             ...  \n",
       "470047     0.196128  \n",
       "470048     0.000219  \n",
       "470049     0.000387  \n",
       "470050     0.000586  \n",
       "470051     0.022108  \n",
       "\n",
       "[470052 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "320fc194-0a67-4157-89b6-501e8031aefd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(470052, 10000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c447ad3-8846-42cc-a2ac-90fc264e1308",
   "metadata": {},
   "source": [
    "# Build the Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5939df29-7703-4da3-893b-7c05a67a9434",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc242320-a063-42e3-98c8-19b7fa7bebdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac9d0196-f79f-4d8f-8fce-7566d1ff7142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic_sent.shape: torch.Size([128, 32])\n",
      "neutral_sent.shape: torch.Size([128, 32])\n"
     ]
    }
   ],
   "source": [
    "# let's check if shape and everything is ok\n",
    "for batch in train_dataloader:\n",
    "    toxic_sent, neutral_sent = batch\n",
    "    print(\"toxic_sent.shape:\", toxic_sent.shape)\n",
    "    print(\"neutral_sent.shape:\", neutral_sent.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51c2c3c9-7df9-48a5-a20a-5b80b36aec80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc46dfbe-d834-48e7-ad09-c49c2a676386",
   "metadata": {},
   "source": [
    "# Load the Model\n",
    "\n",
    "- EncoderDecoder (Seq2Seq) with Attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1b9c17a-0069-481e-a76c-240d46bf10a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.attention.encoder import Encoder\n",
    "from src.models.attention.decoder import Decoder\n",
    "from src.models.attention.attention import Attention\n",
    "from src.models.attention import Seq2SeqAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "451a159a-dae5-487c-be50-c87b7407add1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Encoder\n",
    "enc_input_dim = len(enc_vocab)\n",
    "enc_embed_dim = 128\n",
    "enc_hidden_dim = 256\n",
    "enc_dropout = 0.5\n",
    "enc_padding_idx = enc_vocab['<pad>']\n",
    "\n",
    "## Decoder\n",
    "dec_output_dim = len(dec_vocab)\n",
    "dec_embed_dim = 128\n",
    "dec_hidden_dim = 256\n",
    "dec_dropout = 0.5\n",
    "dec_padding_idx = dec_vocab['<pad>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b48dfdf-ba48-477c-ac62-2c512a69d86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the encoder and decoder for our model\n",
    "encoder = Encoder(\n",
    "    input_dim=enc_input_dim,\n",
    "    embed_dim=enc_embed_dim,\n",
    "    hidden_dim=enc_hidden_dim,\n",
    "    dec_hidden_dim=dec_hidden_dim,\n",
    "    dropout=enc_dropout,\n",
    "    vocab=enc_vocab,\n",
    "    padding_idx=enc_padding_idx,\n",
    ").to(device)\n",
    "\n",
    "attention = Attention(\n",
    "    enc_hidden_dim,\n",
    "    dec_hidden_dim,\n",
    ")\n",
    "\n",
    "decoder = Decoder(\n",
    "    output_dim=dec_output_dim,\n",
    "    embed_dim=dec_embed_dim,\n",
    "    hidden_dim=dec_hidden_dim,\n",
    "    attention=attention,\n",
    "    enc_hidden_dim=enc_hidden_dim,\n",
    "    dropout=dec_dropout,\n",
    "    vocab=dec_vocab,\n",
    "    padding_idx=dec_padding_idx,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fb68fa7c-f8ee-4667-9b20-a638de4fe311",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = float('inf')\n",
    "\n",
    "model = Seq2SeqAttention(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    device=device,\n",
    "    max_sent_size=MAX_SENT_SIZE,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b4ce097-26f1-4aeb-a5e4-5985e6b6c885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters in model: 13.0M\n"
     ]
    }
   ],
   "source": [
    "from src.models.utils import count_parameters\n",
    "\n",
    "print(f\"number of parameters in model: {count_parameters(model)//1e6}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "088f446e-d95e-4812-949d-a3a5eb20ab6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=dec_vocab['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae355174-b99f-403c-b926-def9bd694c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 1: 100%|██████████| 3673/3673 [05:26<00:00, 11.25it/s, loss=3.29]\n",
      "Evaluating 1: 100%|██████████| 79/79 [00:02<00:00, 34.17it/s, loss=5.2] \n",
      "Training 2: 100%|██████████| 3673/3673 [05:26<00:00, 11.25it/s, loss=2.6] \n",
      "Evaluating 2: 100%|██████████| 79/79 [00:02<00:00, 33.88it/s, loss=5.29]\n",
      "Training 3: 100%|██████████| 3673/3673 [05:26<00:00, 11.24it/s, loss=2.42]\n",
      "Evaluating 3: 100%|██████████| 79/79 [00:02<00:00, 33.99it/s, loss=5.31]\n",
      "Training 4: 100%|██████████| 3673/3673 [05:27<00:00, 11.21it/s, loss=2.31]\n",
      "Evaluating 4: 100%|██████████| 79/79 [00:02<00:00, 34.07it/s, loss=5.27]\n",
      "Training 5: 100%|██████████| 3673/3673 [05:27<00:00, 11.21it/s, loss=2.22]\n",
      "Evaluating 5: 100%|██████████| 79/79 [00:02<00:00, 33.92it/s, loss=5.31]\n",
      "Training 6: 100%|██████████| 3673/3673 [05:26<00:00, 11.23it/s, loss=2.16]\n",
      "Evaluating 6: 100%|██████████| 79/79 [00:02<00:00, 33.61it/s, loss=5.27]\n",
      "Training 7: 100%|██████████| 3673/3673 [05:27<00:00, 11.20it/s, loss=2.11]\n",
      "Evaluating 7: 100%|██████████| 79/79 [00:02<00:00, 33.92it/s, loss=5.28]\n",
      "Training 8: 100%|██████████| 3673/3673 [05:27<00:00, 11.21it/s, loss=2.06]\n",
      "Evaluating 8: 100%|██████████| 79/79 [00:02<00:00, 33.91it/s, loss=5.26]\n",
      "Training 9: 100%|██████████| 3673/3673 [05:27<00:00, 11.21it/s, loss=2.03]\n",
      "Evaluating 9: 100%|██████████| 79/79 [00:02<00:00, 33.08it/s, loss=5.27]\n",
      "Training 10: 100%|██████████| 3673/3673 [05:27<00:00, 11.22it/s, loss=1.99]\n",
      "Evaluating 10: 100%|██████████| 79/79 [00:02<00:00, 34.07it/s, loss=5.27]\n",
      "Training 11: 100%|██████████| 3673/3673 [05:27<00:00, 11.20it/s, loss=1.96]\n",
      "Evaluating 11: 100%|██████████| 79/79 [00:02<00:00, 33.69it/s, loss=5.22]\n",
      "Training 12: 100%|██████████| 3673/3673 [05:27<00:00, 11.21it/s, loss=1.94]\n",
      "Evaluating 12: 100%|██████████| 79/79 [00:02<00:00, 34.21it/s, loss=5.25]\n",
      "Training 13: 100%|██████████| 3673/3673 [05:27<00:00, 11.22it/s, loss=1.91]\n",
      "Evaluating 13: 100%|██████████| 79/79 [00:02<00:00, 33.85it/s, loss=5.28]\n",
      "Training 14: 100%|██████████| 3673/3673 [05:27<00:00, 11.21it/s, loss=1.89]\n",
      "Evaluating 14: 100%|██████████| 79/79 [00:02<00:00, 33.74it/s, loss=5.26]\n",
      "Training 15: 100%|██████████| 3673/3673 [05:27<00:00, 11.21it/s, loss=1.87]\n",
      "Evaluating 15: 100%|██████████| 79/79 [00:02<00:00, 33.65it/s, loss=5.25]\n",
      "Training 16: 100%|██████████| 3673/3673 [05:27<00:00, 11.21it/s, loss=1.85]\n",
      "Evaluating 16: 100%|██████████| 79/79 [00:02<00:00, 33.55it/s, loss=5.25]\n",
      "Training 17: 100%|██████████| 3673/3673 [05:27<00:00, 11.21it/s, loss=1.83]\n",
      "Evaluating 17: 100%|██████████| 79/79 [00:02<00:00, 33.60it/s, loss=5.23]\n",
      "Training 18: 100%|██████████| 3673/3673 [05:29<00:00, 11.15it/s, loss=1.82]\n",
      "Evaluating 18: 100%|██████████| 79/79 [00:02<00:00, 33.44it/s, loss=5.25]\n",
      "Training 19: 100%|██████████| 3673/3673 [05:27<00:00, 11.21it/s, loss=1.8] \n",
      "Evaluating 19: 100%|██████████| 79/79 [00:02<00:00, 34.00it/s, loss=5.26]\n",
      "Training 20: 100%|██████████| 3673/3673 [05:28<00:00, 11.19it/s, loss=1.79]\n",
      "Evaluating 20: 100%|██████████| 79/79 [00:02<00:00, 33.43it/s, loss=5.22]\n"
     ]
    }
   ],
   "source": [
    "from src.models.train_model import train\n",
    "\n",
    "best_loss = train(\n",
    "    model=model,\n",
    "    loaders=(train_dataloader, val_dataloader),\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    epochs=20,\n",
    "    device=device,\n",
    "    best_loss=best_loss,\n",
    "    ckpt_path=MODEL_CHECKPOINT,\n",
    "    clip_grad=1,\n",
    "    teacher_force={\n",
    "        'value': 0.95,\n",
    "        'gamma': 1.0,\n",
    "        'update_every_n_epoch': 50,\n",
    "    } # first 10 epoch teacher force 1, after it will be turned off\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136929a8-2d0f-4e37-9a0d-ede2a5fe305c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'models/attention2.02.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1bf23d3f-6229-4856-9926-9240d3deee79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic_sent: I am going to start running a breast artery.\n",
      "neutral_sent: I will start the harvest of the mammary artery.\n",
      "predictions:\n",
      "\t1) ['I', 'am', 'going', 'to', 'start', 'running', 'a', 'breast', 'artery', '.', '<eos>']\n",
      "\t2) ['I', 'am', 'going', 'to', 'start', 'running', 'a', '<unk>', 'artery', '.', '<eos>']\n",
      "\t3) ['I', 'am', 'going', 'to', 'start', 'running', 'the', 'breast', 'artery', '.', '<eos>']\n",
      "\n",
      "\n",
      "toxic_sent: stop playing dumb with us!\n",
      "neutral_sent: stop playing mute with us!\n",
      "predictions:\n",
      "\t1) ['stop', 'playing', 'games', 'with', 'us', '!', '<eos>']\n",
      "\t2) ['stop', 'messing', 'with', 'us', '!', '<eos>']\n",
      "\t3) ['stop', 'playing', 'with', 'us', '!', '<eos>']\n",
      "\n",
      "\n",
      "toxic_sent: damn, i broke the door.\n",
      "neutral_sent: i broke the frame.\n",
      "predictions:\n",
      "\t1) ['hell', ',', 'i', 'broke', 'the', 'door', '.', '<eos>']\n",
      "\t2) ['i', 'broke', 'the', 'door', '.', '<eos>']\n",
      "\t3) ['hell', ',', 'i', 'broke', 'my', 'door', '.', '<eos>']\n",
      "\n",
      "\n",
      "toxic_sent: everybody shut up!\n",
      "neutral_sent: come on, silence all!\n",
      "predictions:\n",
      "\t1) ['everybody', 'quiet', '!', '<eos>']\n",
      "\t2) ['everybody', 'i', 'quiet', '!', '<eos>']\n",
      "\t3) ['everyone', 'quiet', '!', '<eos>']\n",
      "\n",
      "\n",
      "toxic_sent: i did a good job until richard broke up with all that crap about betty, like his girl on a rough ride.\n",
      "neutral_sent: I would been doing okay until he would made that crack about betty being his girl for the rough stuff.\n",
      "predictions:\n",
      "\t1) ['i', 'did', 'a', 'good', 'job', 'until', 'richard', 'broke', 'up', 'with', 'betty', ',', 'like', 'his', 'girl', 'on', 'a', 'rough', 'ride', '.', '<eos>']\n",
      "\t2) ['I', 'have', 'done', 'a', 'good', 'job', 'until', 'richard', 'broke', 'up', 'with', 'betty', ',', 'like', 'his', 'girl', 'on', 'a', 'rough', 'ride', '.', '<eos>']\n",
      "\t3) ['I', 'have', 'done', 'a', 'good', 'job', 'until', 'richard', 'broke', 'up', 'with', 'betty', ',', 'like', 'his', 'girlfriend', 'on', 'a', 'rough', 'ride', '.', '<eos>']\n",
      "\n",
      "\n",
      "toxic_sent: hello, tall, dark hater.\n",
      "neutral_sent: hey, tall, dark and jaundiced.\n",
      "predictions:\n",
      "\t1) ['hello', ',', 'tall', ',', 'dark', '<unk>', '.', '<eos>']\n",
      "\t2) ['hello', ',', 'tall', ',', 'dark', 'haired', '.', '<eos>']\n",
      "\t3) ['tall', ',', 'tall', ',', 'dark', 'haired', '.', '<eos>']\n",
      "\n",
      "\n",
      "toxic_sent: lift up your head like that . and the breasts . breasts up and forward.\n",
      "neutral_sent: raise your head like this and your chest your chest up and in front.\n",
      "predictions:\n",
      "\t1) ['raise', 'your', 'head', 'like', 'that', '.', 'breasts', 'and', 'breasts', '.', '<eos>']\n",
      "\t2) ['raise', 'your', 'head', 'like', 'that', 'and', 'breasts', 'chest', '.', '<eos>']\n",
      "\t3) ['raise', 'your', 'head', 'like', 'that', ',', 'and', 'chest', 'breasts', '.', '<eos>']\n",
      "\n",
      "\n",
      "toxic_sent: we will eliminate anyone who gets in the way.\n",
      "neutral_sent: we eliminate anyone that gets in our way.\n",
      "predictions:\n",
      "\t1) ['eliminate', 'anyone', 'who', 'gets', 'in', 'the', 'way', '.', '<eos>']\n",
      "\t2) ['we', 'eliminate', 'anyone', 'who', 'gets', 'in', 'the', 'way', '.', '<eos>']\n",
      "\t3) ['we', 'eliminate', 'eliminate', 'anyone', 'who', 'gets', 'in', 'the', 'way', '.', '<eos>']\n",
      "\n",
      "\n",
      "toxic_sent: all right, so, tell me, what is the story with stark and these leviathan jerks?\n",
      "neutral_sent: so tell me, how is it with stark and those of leviathan i party?\n",
      "predictions:\n",
      "\t1) ['okay', ',', 'so', 'tell', 'me', ',', 'what', 'is', 'the', 'story', 'with', 'stark', 'stark', 'and', 'these', '<unk>', '<unk>', '?', '<eos>']\n",
      "\t2) ['okay', ',', 'so', 'tell', 'me', ',', 'what', 'is', 'the', 'story', 'with', 'stark', 'stark', 'and', '<unk>', '<unk>', '?', '<eos>']\n",
      "\t3) ['okay', ',', 'so', 'tell', 'me', ',', 'what', 'is', 'with', 'the', 'stark', 'and', 'stark', '<unk>', '?', '<eos>']\n",
      "\n",
      "\n",
      "toxic_sent: damn, she is got my sword.\n",
      "neutral_sent: he is got my swords.\n",
      "predictions:\n",
      "\t1) ['she', 'is', 'got', 'my', 'sword', '.', '<eos>']\n",
      "\t2) ['hell', ',', 'she', 'is', 'got', 'my', 'sword', '.', '<eos>']\n",
      "\t3) ['she', 'is', 'got', 'swords', '.', '<eos>']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "detokenizer = TreebankWordDetokenizer()\n",
    "\n",
    "# let's see how our model works\n",
    "num_examples = 10\n",
    "num_sentence = 3\n",
    "dataset = train_dataset\n",
    "for _ in range(num_examples):\n",
    "    idx = np.random.randint(0, len(dataset))\n",
    "    toxic_sent = detokenizer.detokenize(dataset.df.loc[idx, 'toxic_sent'])\n",
    "    neutral_sent = detokenizer.detokenize(dataset.df.loc[idx, 'neutral_sent'])\n",
    "    \n",
    "    print('toxic_sent:', toxic_sent)\n",
    "    print('neutral_sent:', neutral_sent)\n",
    "    preds = model.predict(\n",
    "        toxic_sent,\n",
    "        beam=True,\n",
    "        beam_search_num_candidates=num_sentence,\n",
    "        post_process_text=False,\n",
    "    ) # let's use beam search\n",
    "    print(\"predictions:\")\n",
    "    for i in range(num_sentence):\n",
    "        print(f\"\\t{i+1})\", preds[i])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7dc1b6ad-901d-4582-84c8-79ba34fdcebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqAttention(\n",
       "  (encoder): Encoder(\n",
       "    (vocab): Vocab()\n",
       "    (embedding): Embedding(10000, 128, padding_idx=1)\n",
       "    (rnn): LSTM(128, 256, batch_first=True, bidirectional=True)\n",
       "    (fc_hidden): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (fc_cell): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (vocab): Vocab()\n",
       "    (embedding): Embedding(10000, 128, padding_idx=1)\n",
       "    (rnn): LSTM(640, 256, batch_first=True)\n",
       "    (attention): Attention(\n",
       "      (attn): Linear(in_features=768, out_features=256, bias=True)\n",
       "      (v): Linear(in_features=256, out_features=1, bias=False)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (fc_out): Linear(in_features=896, out_features=10000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's load the model and predict\n",
    "model = torch.load(MODEL_CHECKPOINT)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e3f96417-66e1-4061-ba49-55207e953142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic_sent: i will collect your heads.\n",
      "neutral_sent: I will get your head.\n",
      "predictions:\n",
      "\t1) ['I', 'will', 'take', 'your', 'heads', '.', '<eos>']\n",
      "\t2) ['I', 'will', 'be', 'your', 'heads', '.', '<eos>']\n",
      "\t3) ['I', 'am', 'going', 'to', 'take', 'your', 'heads', '.', '<eos>']\n",
      "\n",
      "\n",
      "toxic_sent: leave one ship and leave, or you will die.\n",
      "neutral_sent: i leave a vessel and get out, or muri i here.\n",
      "predictions:\n",
      "\t1) ['leave', 'one', 'ship', 'and', 'leave', ',', 'or', 'you', 'will', 'die', '.', '<eos>']\n",
      "\t2) ['leave', 'the', 'ship', 'and', 'leave', ',', 'or', 'you', 'will', 'die', '.', '<eos>']\n",
      "\t3) ['leave', 'one', 'ship', ',', 'leave', ',', 'or', 'you', 'will', 'die', '.', '<eos>']\n",
      "\n",
      "\n",
      "toxic_sent: well, it is going to be both our asses if you are wrong.\n",
      "neutral_sent: if you are wrong, it is going to cost us both.\n",
      "predictions:\n",
      "\t1) ['well', ',', 'it', 'is', 'going', 'to', 'be', 'both', 'if', 'you', 'are', 'wrong', '.', '<eos>']\n",
      "\t2) ['well', ',', 'it', 'will', 'be', 'both', 'if', 'you', 'are', 'wrong', '.', '<eos>']\n",
      "\t3) ['well', ',', 'it', 'is', 'going', 'to', 'be', 'both', 'of', 'our', 'butts', 'if', 'you', 'are', 'wrong', '.', '<eos>']\n",
      "\n",
      "\n",
      "toxic_sent: if you come back here, he will die.\n",
      "neutral_sent: you come back in, he dies.\n",
      "predictions:\n",
      "\t1) ['if', 'you', 'come', 'back', 'here', ',', 'he', 'dies', '.', '<eos>']\n",
      "\t2) ['if', 'you', 'come', 'back', 'here', ',', 'he', 'will', 'die', '.', '<eos>']\n",
      "\t3) ['if', 'you', 'come', 'back', ',', 'he', 'dies', '.', '<eos>']\n",
      "\n",
      "\n",
      "toxic_sent: you like being a fucking hero, huh?\n",
      "neutral_sent: you feel like a hero now?\n",
      "predictions:\n",
      "\t1) ['you', 'like', 'being', 'hero', ',', 'huh', '?', '<eos>']\n",
      "\t2) ['you', 'like', 'being', 'a', 'hero', ',', 'huh', '?', '<eos>']\n",
      "\t3) ['you', 'like', 'a', 'hero', ',', 'huh', '?', '<eos>']\n",
      "\n",
      "\n",
      "toxic_sent: my breast for your sword point.\n",
      "neutral_sent: my chest for the blade of your sword.\n",
      "predictions:\n",
      "\t1) ['my', 'breast', 'for', 'your', 'sword', '.', '<eos>']\n",
      "\t2) ['my', 'breast', 'for', 'your', 'sword', 'point', '.', '<eos>']\n",
      "\t3) ['my', 'breast', 'for', 'my', 'sword', '.', '<eos>']\n",
      "\n",
      "\n",
      "toxic_sent: he is my innocent son of a bitch!\n",
      "neutral_sent: it is my innocent boy!\n",
      "predictions:\n",
      "\t1) ['he', 'is', 'my', 'innocent', 'son', '!', '<eos>']\n",
      "\t2) ['it', 'is', 'my', 'innocent', 'son', '!', '<eos>']\n",
      "\t3) ['my', 'innocent', 'son', '!', '<eos>']\n",
      "\n",
      "\n",
      "toxic_sent: i destroyed them.\n",
      "neutral_sent: i ruined them.\n",
      "predictions:\n",
      "\t1) ['i', 'destroyed', 'them', '.', '<eos>']\n",
      "\t2) ['i', 'destroyed', 'them', 'them', '.', '<eos>']\n",
      "\t3) ['i', 'destroyed', 'them', '!', '<eos>']\n",
      "\n",
      "\n",
      "toxic_sent: i know, nick fisher spent eight years in jail because he wanted to fuck a 14 year old cheerleader.\n",
      "neutral_sent: i know this because nick fisher spent eight years in dade correctional for forcing himself on a 14 year old cheerleader.\n",
      "predictions:\n",
      "\t1) ['i', 'know', ',', 'nick', 'two', 'years', 'ago', 'years', 'ago', 'because', 'he', 'wanted', 'to', 'get', 'a', '14', 'year', 'old', '.', '<eos>']\n",
      "\t2) ['i', 'know', ',', 'nick', 'two', 'years', 'ago', 'years', 'ago', 'because', 'he', 'wanted', 'to', '14', 'a', '14', 'year', 'old', '.', '<eos>']\n",
      "\t3) ['i', 'know', ',', 'nick', 'two', 'years', 'ago', 'years', 'ago', 'because', 'he', 'wanted', 'to', 'get', 'a', '14', 'year', 'old', 'year', 'old', '.', '<eos>']\n",
      "\n",
      "\n",
      "toxic_sent: sorry, did you want more on your neck?\n",
      "neutral_sent: I am sorry, did you want more neck?\n",
      "predictions:\n",
      "\t1) ['sorry', ',', 'did', 'you', 'want', 'more', 'on', 'your', 'neck', '?', '<eos>']\n",
      "\t2) ['sorry', ',', 'did', 'you', 'want', 'more', 'of', 'your', 'neck', '?', '<eos>']\n",
      "\t3) ['sorry', ',', 'did', 'you', 'want', 'more', '<unk>', 'on', 'your', 'neck', '?', '<eos>']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "detokenizer = TreebankWordDetokenizer()\n",
    "\n",
    "# let's see how our model works\n",
    "num_examples = 10\n",
    "num_sentence = 3\n",
    "dataset = train_dataset\n",
    "for _ in range(num_examples):\n",
    "    idx = np.random.randint(0, len(dataset))\n",
    "    toxic_sent = detokenizer.detokenize(dataset.df.loc[idx, 'toxic_sent'])\n",
    "    neutral_sent = detokenizer.detokenize(dataset.df.loc[idx, 'neutral_sent'])\n",
    "    \n",
    "    print('toxic_sent:', toxic_sent)\n",
    "    print('neutral_sent:', neutral_sent)\n",
    "    preds = model.predict(\n",
    "        toxic_sent,\n",
    "        beam=True,\n",
    "        beam_search_num_candidates=num_sentence,\n",
    "        post_process_text=False,\n",
    "    ) # let's use beam search\n",
    "    print(\"predictions:\")\n",
    "    for i in range(num_sentence):\n",
    "        print(f\"\\t{i+1})\", preds[i])\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
